{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6595238b",
   "metadata": {},
   "source": [
    "# Link to ChatGPT\n",
    "https://chatgpt.com/share/6736b596-2e54-800e-bd15-f1559d7b0c35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d46050",
   "metadata": {},
   "source": [
    "# 1\n",
    "## 1. Simple Linear Regression vs. Multiple Linear Regression\n",
    "#### Simple Linear Regression\n",
    "- Models the relationship between one predictor variable ($X$) and the outcome variable (Y). The form is: \n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 x$\n",
    "\n",
    "- $\\beta_0$ is the intercept, and $\\beta_1$ is the coefficient showing the effect of $X$ on $Y$. \n",
    "\n",
    "#### Multiple Linear Regression\n",
    "- Extends this to include multiple predictor variables ($X_1, X_2, ..., X_n$), allowing for a more complex and nuanced model of the outcome variable. The form is:\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n$\n",
    "\n",
    "- By using more predictors, Multiple Linear Regression can explain variation in $Y$ that couldn't be captured by a single predictor alone.\n",
    "- Offers a better fit to the data if the additional variables are relevant.\n",
    "\n",
    "## 2. Continuous vs. Indicator Variables in Simple Linear Regression\n",
    "#### Continuous Variable\n",
    "- Takes on a range of values, leading to a model where:\n",
    "    - Each unit change in $X$ affects $Y$ by a constant amount. \n",
    "    - For instance, in $Y = \\beta_0 + \\beta_1 X, \\beta_1$ represents the effect of each unit increase in $X$ on $Y$.\n",
    "\n",
    "#### Indicator (Binary) Variable\n",
    "- Takes on values of 0 or 1, representing two distinct groups (e.g., \"treatment\" or \"control\".) The model form is:\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 1(X)$\n",
    "\n",
    "- $1(X)$ equals 1 for one group and 0 for the other\n",
    "- $\\beta_0$ : outcome for the reference group $(1(X)=0$\n",
    "- $\\beta_1$ : difference in $Y$ between the two groups\n",
    "\n",
    "## 3. Introducing an Indicator Variable in Multiple Linear Regression\n",
    "- When a continuous variable $X_1$ and an indicator variable $1(X_2)$ are used together in Multiple Linear Regression, the model form is:\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 1(X_2)$\n",
    "\n",
    "- $\\beta_2$: caputres the difference between the two groups defined by $1(X_2)$\n",
    "- $\\beta_1$: shows the effect of $X_1$ on $Y$, regardless of group\n",
    "- Allows the model to differentiate the outcome based on $1(X_2)$ while still accounting for the influence of $X_1$.\n",
    "\n",
    "## 4. Adding an Interaction Between a Continuous and an Indicator Variable in Multiple Linear Regression\n",
    "- An interaction term between a continuous variable $X_1$ and an indicator variable $1(X_2)$ captures the possibility that the effect of $X_1$ on $Y$ differs by the level of $1(X_2)$. The model form is:\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 1(X_2) + \\beta_3 (X_1 \\times\n",
    "1(X_2))$\n",
    "\n",
    "- $\\beta_3$: represents the additional change in the slope of $X_1$ when $1(X_2) = 1$\n",
    "- Enables the model to allow for different relationships between $X_!$ and $Y$ based on the value of $1(X_2)$\n",
    "\n",
    "## 5. Multiple Linear Regression with Only Indicator Variables for a Categorical Variable\n",
    "- For a categorical variable with $k$ categories (e.g., regions: North, South, East, West), we use $k - 1$ indicator variables to represent these categories.\n",
    "- Suppose that we encode North, South, and East with indicator variables, while West serves as the baseline group. The model form is: \n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 1(\\text{North}) + \\beta_2 1(\\text{South}) + \\beta_3 1(\\text{East})$\n",
    "\n",
    "- $\\beta_0$: represents the baseline outcome for West\n",
    "- $\\beta_1, \\beta_2, and \\beta_3$: capture the difference in $Y$ for North, South, and East, compared to West\n",
    "- This encoding (often called \"dummy coding\") avoids redundancy and multicollinearity by:\n",
    "    - Representing cateogry membership with binary (0 or 1) indicator variables\n",
    "    - Makes it easier to interpret the unique effect of each category relative to the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c2696",
   "metadata": {},
   "source": [
    "# ChatGPT Summary\n",
    "\n",
    "In this interaction, we discussed key concepts in linear regression, focusing on differences between Simple and Multiple Linear Regression. We covered how Simple Linear Regression models a single predictor, while Multiple Linear Regression incorporates multiple predictors, allowing for more nuanced models. We also reviewed the use of continuous versus indicator variables, noting that indicator variables represent distinct groups and allow categorical information in the model.\n",
    "\n",
    "We explored how adding an indicator variable to a continuous variable affects model behavior, enabling the model to differentiate outcomes by group. We discussed interaction terms in Multiple Linear Regression, where combining continuous and indicator variables allows varying relationships between predictors and outcomes across groups. Finally, we examined Multiple Linear Regression with only indicator variables for categorical predictors, including the baseline category concept, and the need for \\(k-1\\) indicators to represent \\(k\\) categories. Equations for each form were provided in LaTeX format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2de507",
   "metadata": {},
   "source": [
    "# 2\n",
    "## Identifying Variables and Interactions\n",
    "For this scenario:\n",
    "    \n",
    "   - Outcome Variable ($Y$): Total sales from the advertising campaigns\n",
    "   \n",
    "   - Predictor Variables:\n",
    "        - $X_{\\text{TV}}$: Amount spent on TV advertising\n",
    "        - $X_{\\text{Online}}$: Amount spent on online advertising\n",
    "        \n",
    "## Model Forms\n",
    "#### 1. Without Interaction (Additive Model):\n",
    "- The effect of TV and online advertising on sales\n",
    "    => indepdent of each other\n",
    "- The model assumes each advertising type contributes to sales without influencing the effectiveness of the other\n",
    "\n",
    "$Y = \\beta_0 + \\beta_{\\text{TV}} X_{\\text{TV}} + \\beta_{\\text{Online}} X_{\\text{Online}}$\n",
    "\n",
    "#### 2. With Interaction (Synergistic Model):\n",
    "- The interaction between TV and online advertising is included\n",
    "- Captures the idea that the effectiveness of one advertising medium could depend on the spending in the other\n",
    "\n",
    "$Y = \\beta_0 + \\beta_{\\text{TV}} X_{\\text{TV}} + \\beta_{\\text{Online}} X_{\\text{Online}} + \\beta_{\\text{TV} \\times \\text{Online}} (X_{\\text{TV}} \\times X_{\\text{Online}})$\n",
    "\n",
    "## Explanation of Predictions\n",
    "- **Additive Model**: \n",
    "    - The model without interaction predicts total sales as a simple sum of individual effects from each type of advertising\n",
    "    - This model works if both ad campaigns have independent effect on sales\n",
    "    \n",
    "- **Synergistic Model**:\n",
    "    - The model with interaction adds a term for $\\beta_{\\text{TV} \\times \\text{Online}} (X_{\\text{TV}} \\times X_{\\text{Online}})$\n",
    "    - Captures an interaction effect\n",
    "    - The impact of TV ads on sales may increase or decrease depending on the online ad budget and vice versa. \n",
    "    - The model predicts sales to vary more dynamically, as it accounts for the interplay between the two types of advertising\n",
    "    \n",
    "## Updating the Model for Binary Indicator Variables\n",
    "If instead we categorize the advertisement budgets as either \"High\" or \"Low\", we replace the continuous variables $X_{\\text{TV}}$ and  $X_{\\text{Online}}$ with binary indicators: \n",
    "- **Binary Variables**:\n",
    "    - $1(X_{\\text{TV}} = \\text{High})$: Indicator for \"High\" TV ad budget (1 if High, 0 if Low).\n",
    "    -  $1(X_{\\text{Online}} = \\text{High})$ : Indicator for \"High\" online ad budget (1 if High, 0 if Low).\n",
    "\n",
    "**Binary Model Forms**\n",
    "1. **Without Interaction**:\n",
    "$Y = \\beta_0 + \\beta_{\\text{TV}} \\cdot 1(X_{\\text{TV}} = \\text{High}) + \\beta_{\\text{Online}} \\cdot 1(X_{\\text{Online}} = \\text{High})$\n",
    "\n",
    "2. **With Interaction**:\n",
    "$Y = \\beta_0 + \\beta_{\\text{TV}} \\cdot 1(X_{\\text{TV}} = \\text{High}) + \\beta_{\\text{Online}} \\cdot 1(X_{\\text{Online}} = \\text{High}) + \\beta_{\\text{TV} \\times \\text{Online}} \\cdot \\big(1(X_{\\text{TV}} = \\text{High}) \\times 1(X_{\\text{Online}} = \\text{High})\\big)$\n",
    "\n",
    "In these models:\n",
    "- **Without Interaction**:\n",
    "    - The model will predict sales for four possible cases independetly ( both High, both Low, only TV High, only Online High)\n",
    "    - Each effect on sales is additive\n",
    "    \n",
    "- **With Interaction**:\n",
    "    - An additional interaction term accounts for cases where:\n",
    "        - Both budgets are simultaneously, potentially increasing or decreasing the expected sales beyong what the individual effects would predict\n",
    "    - This model might be more accurate if:\n",
    "        - having high budgets in both areas creates a synergistic boost in sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77cdba",
   "metadata": {},
   "source": [
    "# ChatGPT Summary\n",
    "We discussed the setup and interpretation of multiple linear regression models with and without interaction terms. Specifically, we examined a scenario where advertising budgets for TV and online platforms could impact sales. We started with continuous variables to explore additive and synergistic (interaction) models, where the effect of one advertising platform on sales could depend on the budget of the other. I provided the linear forms for both additive and interaction models in LaTeX.\n",
    "\n",
    "Next, we considered a version using binary indicators for \"High\" or \"Low\" ad budgets. I outlined the binary indicator variables and provided the updated linear forms for models with and without interaction terms. Finally, I provided LaTeX code for both the model forms and definitions of the binary indicators for clear documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fd3f9",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2583e082",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "94f2709d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70/2969168346.py:6: DtypeWarning:\n",
      "\n",
      "Columns (408,1001,1002,1006,1007,1008,1080,1113,1115,1116,1117,1118,1119,1120,1121,1124,1125,1126,1127,1128,1213,1214,1215,1216,1217,1218,1342,1343,1344,1345,1346,1347,1348,1349,1390,1391,1393,1463,1549,1552,1555,1558,1561) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNIQUE_id</th>\n",
       "      <th>UNIQUE_num_records</th>\n",
       "      <th>ELIGIBLE_consent</th>\n",
       "      <th>GEO_residence_canada</th>\n",
       "      <th>GEO_province</th>\n",
       "      <th>DEMO_age</th>\n",
       "      <th>DEMO_gender</th>\n",
       "      <th>DEMO_identity_vetrans</th>\n",
       "      <th>DEMO_identity_indigenous</th>\n",
       "      <th>DEMO_identity_lgbtq</th>\n",
       "      <th>...</th>\n",
       "      <th>PSYCH_body_self_image_questionnaire_height_dissatisfaction_score</th>\n",
       "      <th>PSYCH_body_self_image_questionnaire_fatness_evaluation_score</th>\n",
       "      <th>PSYCH_body_self_image_questionnaire_negative_affect_score</th>\n",
       "      <th>PSYCH_body_self_image_questionnaire_social_dependence_score</th>\n",
       "      <th>PSYCH_big_five_inventory_agreeable_score</th>\n",
       "      <th>PSYCH_big_five_inventory_conscientious_score</th>\n",
       "      <th>PSYCH_big_five_inventory_extraverted_score</th>\n",
       "      <th>PSYCH_big_five_inventory_neurotic_score</th>\n",
       "      <th>PSYCH_big_five_inventory_open_score</th>\n",
       "      <th>REMOVE_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cscs_00001</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Non-binary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sexual or gender minorities (e.g., LGBTQ2+)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cscs_00002</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>69.0</td>\n",
       "      <td>Woman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Selected</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cscs_00003</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>56.0</td>\n",
       "      <td>Woman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Selected</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cscs_00005</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Woman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Selected</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cscs_00006</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>Not Selected</td>\n",
       "      <td>Indigenous peoples (e.g., First Nations, Métis...</td>\n",
       "      <td>Sexual or gender minorities (e.g., LGBTQ2+)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    UNIQUE_id  UNIQUE_num_records ELIGIBLE_consent GEO_residence_canada  \\\n",
       "0  cscs_00001                   1              Yes                  Yes   \n",
       "1  cscs_00002                   1              Yes                  Yes   \n",
       "2  cscs_00003                   1              Yes                  Yes   \n",
       "3  cscs_00005                   1              Yes                  Yes   \n",
       "4  cscs_00006                   1              Yes                  Yes   \n",
       "\n",
       "       GEO_province  DEMO_age DEMO_gender DEMO_identity_vetrans  \\\n",
       "0  British Columbia      71.0  Non-binary                   NaN   \n",
       "1           Ontario      69.0       Woman                   NaN   \n",
       "2            Quebec      56.0       Woman                   NaN   \n",
       "3               NaN      54.0       Woman                   NaN   \n",
       "4           Ontario      30.0         Man          Not Selected   \n",
       "\n",
       "                            DEMO_identity_indigenous  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  Indigenous peoples (e.g., First Nations, Métis...   \n",
       "\n",
       "                           DEMO_identity_lgbtq  ...  \\\n",
       "0  Sexual or gender minorities (e.g., LGBTQ2+)  ...   \n",
       "1                                 Not Selected  ...   \n",
       "2                                 Not Selected  ...   \n",
       "3                                 Not Selected  ...   \n",
       "4  Sexual or gender minorities (e.g., LGBTQ2+)  ...   \n",
       "\n",
       "  PSYCH_body_self_image_questionnaire_height_dissatisfaction_score  \\\n",
       "0                                                NaN                 \n",
       "1                                                3.0                 \n",
       "2                                                NaN                 \n",
       "3                                                NaN                 \n",
       "4                                                NaN                 \n",
       "\n",
       "  PSYCH_body_self_image_questionnaire_fatness_evaluation_score  \\\n",
       "0                                                NaN             \n",
       "1                                                8.0             \n",
       "2                                                NaN             \n",
       "3                                                NaN             \n",
       "4                                                NaN             \n",
       "\n",
       "  PSYCH_body_self_image_questionnaire_negative_affect_score  \\\n",
       "0                                                NaN          \n",
       "1                                                3.0          \n",
       "2                                                NaN          \n",
       "3                                                NaN          \n",
       "4                                                NaN          \n",
       "\n",
       "  PSYCH_body_self_image_questionnaire_social_dependence_score  \\\n",
       "0                                                NaN            \n",
       "1                                                3.0            \n",
       "2                                                NaN            \n",
       "3                                                NaN            \n",
       "4                                                NaN            \n",
       "\n",
       "  PSYCH_big_five_inventory_agreeable_score  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                     28.0   \n",
       "4                                      NaN   \n",
       "\n",
       "  PSYCH_big_five_inventory_conscientious_score  \\\n",
       "0                                          NaN   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                         34.0   \n",
       "4                                          NaN   \n",
       "\n",
       "  PSYCH_big_five_inventory_extraverted_score  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                       30.0   \n",
       "4                                        NaN   \n",
       "\n",
       "  PSYCH_big_five_inventory_neurotic_score PSYCH_big_five_inventory_open_score  \\\n",
       "0                                     NaN                                 NaN   \n",
       "1                                     NaN                                 NaN   \n",
       "2                                     NaN                                 NaN   \n",
       "3                                    32.0                                37.0   \n",
       "4                                     NaN                                 NaN   \n",
       "\n",
       "  REMOVE_case  \n",
       "0          No  \n",
       "1          No  \n",
       "2          No  \n",
       "3          No  \n",
       "4          No  \n",
       "\n",
       "[5 rows x 1794 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/jovyan/CSCS_data_anon.csv'  # Make sure this is the correct path\n",
    "survey_data = pd.read_csv(file_path)\n",
    "\n",
    "# Check the first few rows of the dataset to understand its structure\n",
    "survey_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321171de",
   "metadata": {},
   "source": [
    "## Step 2: Convert Categorical Variables to Binary (if necessary)\n",
    "Convert categorical variables like `DEMO_gender` and `DEMO_identity_indigenous` into binary format, which is required for logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c63f2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert gender and indigenous identity to binary format\n",
    "survey_data['DEMO_gender_binary'] = survey_data['DEMO_gender'].map({'Man': 1, 'Woman': 0, 'Non-binary': 0}).fillna(0)\n",
    "survey_data['DEMO_identity_indigenous_binary'] = survey_data['DEMO_identity_indigenous'].apply(lambda x: 1 if pd.notna(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527c7bc",
   "metadata": {},
   "source": [
    "## Step 3: Logistic Regression Formula\n",
    "Define the logistic regression formula and fit the model using `smf.logit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ccca8a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning:\n",
      "\n",
      "Perfect separation or prediction detected, parameter may not be identified\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/base/model.py:595: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/base/model.py:595: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:4465: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>ELIGIBLE_consent[Yes]</td> <th>  No. Observations:  </th>   <td> 10220</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>Logit</td>         <th>  Df Residuals:      </th>   <td> 10216</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                   <td>MLE</td>          <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Fri, 15 Nov 2024</td>    <th>  Pseudo R-squ.:     </th>   <td>   inf</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>02:54:14</td>        <th>  Log-Likelihood:    </th> <td>-0.00020249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>               <td>False</td>         <th>  LL-Null:           </th>  <td>  0.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>nonrobust</td>       <th>  LLR p-value:       </th>   <td> 1.000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   27.6432</td> <td>  583.943</td> <td>    0.047</td> <td> 0.962</td> <td>-1116.865</td> <td> 1172.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DEMO_age</th>                        <td>   -0.1512</td> <td>    7.449</td> <td>   -0.020</td> <td> 0.984</td> <td>  -14.750</td> <td>   14.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DEMO_gender_binary</th>              <td>   -0.1603</td> <td>  198.108</td> <td>   -0.001</td> <td> 0.999</td> <td> -388.445</td> <td>  388.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DEMO_identity_indigenous_binary</th> <td>    0.2485</td> <td>  210.606</td> <td>    0.001</td> <td> 0.999</td> <td> -412.531</td> <td>  413.028</td>\n",
       "</tr>\n",
       "</table><br/><br/>Complete Separation: The results show that there iscomplete separation or perfect prediction.<br/>In this case the Maximum Likelihood Estimator does not exist and the parameters<br/>are not identified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                     & ELIGIBLE\\_consent[Yes] & \\textbf{  No. Observations:  } &     10220    \\\\\n",
       "\\textbf{Model:}                             &         Logit          & \\textbf{  Df Residuals:      } &     10216    \\\\\n",
       "\\textbf{Method:}                            &          MLE           & \\textbf{  Df Model:          } &         3    \\\\\n",
       "\\textbf{Date:}                              &    Fri, 15 Nov 2024    & \\textbf{  Pseudo R-squ.:     } &       inf    \\\\\n",
       "\\textbf{Time:}                              &        02:54:14        & \\textbf{  Log-Likelihood:    } & -0.00020249  \\\\\n",
       "\\textbf{converged:}                         &         False          & \\textbf{  LL-Null:           } &     0.0000   \\\\\n",
       "\\textbf{Covariance Type:}                   &       nonrobust        & \\textbf{  LLR p-value:       } &     1.000    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                            & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                          &      27.6432  &      583.943     &     0.047  &         0.962        &    -1116.865    &     1172.151     \\\\\n",
       "\\textbf{DEMO\\_age}                          &      -0.1512  &        7.449     &    -0.020  &         0.984        &      -14.750    &       14.448     \\\\\n",
       "\\textbf{DEMO\\_gender\\_binary}               &      -0.1603  &      198.108     &    -0.001  &         0.999        &     -388.445    &      388.125     \\\\\n",
       "\\textbf{DEMO\\_identity\\_indigenous\\_binary} &       0.2485  &      210.606     &     0.001  &         0.999        &     -412.531    &      413.028     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Complete Separation: The results show that there iscomplete separation or perfect prediction. \\newline\n",
       " In this case the Maximum Likelihood Estimator does not exist and the parameters \\newline\n",
       " are not identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                             Logit Regression Results                            \n",
       "=================================================================================\n",
       "Dep. Variable:     ELIGIBLE_consent[Yes]   No. Observations:                10220\n",
       "Model:                             Logit   Df Residuals:                    10216\n",
       "Method:                              MLE   Df Model:                            3\n",
       "Date:                   Fri, 15 Nov 2024   Pseudo R-squ.:                     inf\n",
       "Time:                           02:54:14   Log-Likelihood:            -0.00020249\n",
       "converged:                         False   LL-Null:                        0.0000\n",
       "Covariance Type:               nonrobust   LLR p-value:                     1.000\n",
       "===================================================================================================\n",
       "                                      coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          27.6432    583.943      0.047      0.962   -1116.865    1172.151\n",
       "DEMO_age                           -0.1512      7.449     -0.020      0.984     -14.750      14.448\n",
       "DEMO_gender_binary                 -0.1603    198.108     -0.001      0.999    -388.445     388.125\n",
       "DEMO_identity_indigenous_binary     0.2485    210.606      0.001      0.999    -412.531     413.028\n",
       "===================================================================================================\n",
       "\n",
       "Complete Separation: The results show that there iscomplete separation or perfect prediction.\n",
       "In this case the Maximum Likelihood Estimator does not exist and the parameters\n",
       "are not identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Define the logistic regression formula\n",
    "logistic_formula = 'ELIGIBLE_consent ~ DEMO_age + DEMO_gender_binary + DEMO_identity_indigenous_binary'\n",
    "\n",
    "# Fit the logistic regression model\n",
    "log_reg_model = smf.logit(logistic_formula, data=survey_data).fit()\n",
    "\n",
    "# View the summary of the model\n",
    "log_reg_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4898c0b",
   "metadata": {},
   "source": [
    "## Step 4: Interpret the Results\n",
    "Once the model is fitted, we can interpret the coefficients. Since the outcome is binary (0 or 1), the coefficients represent the log-odds of the outcome occuring given the values of the predictors. \n",
    "- **Coefficients**: A positive coefficient means that as the predictor increases, the likelihood of the outcome (i.e., consent) increases, while a negative coefficient suggests the opposite.\n",
    "- **P-values**: Look at the p-values to assess the statistical significance of each predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa36af",
   "metadata": {},
   "source": [
    "## Step 5: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4ac04b79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a new dataset for prediction\n",
    "age_range = np.linspace(survey_data['DEMO_age'].min(), survey_data['DEMO_age'].max(), 100)\n",
    "pred_data = pd.DataFrame({'DEMO_age': age_range, 'DEMO_gender_binary': 0, 'DEMO_identity_indigenous_binary': 0})\n",
    "\n",
    "# Predict probabilities\n",
    "pred_data['predicted_prob'] = log_reg_model.predict(pred_data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(age_range, pred_data['predicted_prob'], label=\"Predicted Probability of Consent\")\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Predicted Probability of Consent')\n",
    "plt.title('Logistic Regression: Predicted Probability of Consent by Age')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f9dd5",
   "metadata": {},
   "source": [
    "# ChatGPT Summary\n",
    "1. **Dataset Overview**: I am working on analyzing the Canadian Social Connection Survey (CSCS) dataset, which includes variables like age, gender, Indigenous identity, and psychological scores, to predict the binary outcome of **survey consent** (`ELIGIBLE_consent`).\n",
    "\n",
    "2. **Data Preparation**: I converted categorical variables (e.g., gender, Indigenous identity) into binary form and used **age** as a continuous predictor.\n",
    "\n",
    "3. **Logistic Regression**: I fitted a logistic regression model using **age**, **gender**, and **Indigenous identity** to predict consent. The model was interpreted through coefficients and p-values.\n",
    "\n",
    "4. **Visualization**: I created a plot to visualize the predicted probabilities of consent based on **age**.\n",
    "\n",
    "5. **Tools Used**: Pandas, Statsmodels (`smf.logit`), and Matplotlib for data manipulation, modeling, and visualization.\n",
    "\n",
    "6. **Conclusion**: The logistic regression model helped identify how demographic factors influence survey consent, with future steps including adding more predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40ac0e",
   "metadata": {},
   "source": [
    "# 4\n",
    "## 1. R-squared: The Proportion of Explained Variability\n",
    "**R-squared** ($R^2$)is a metric that measures how well the model explains the variation in the dependent variable (outcome). It is calculated as: \n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^n(y_i-\\hat y)^2}{\\sum_{i=1}^n(y_i-\\bar y)^2}$$\n",
    "\n",
    "Where:\n",
    "- $y_i$: the observed values\n",
    "- $\\hat y$: the predicted values from the model\n",
    "- $\\bar y$: the mean of the observbed values\n",
    "\n",
    "An **R-squared of 17.6%** means that only 17.6% of the variability in the outcome variable is explained by the predictors in the model. This is often interpreted as a relatively **low explanatory power** - the model doesn't account for most of the variation in the data. \n",
    "\n",
    "However, **R-squared** has limitations:\n",
    "- **R-squared** only measures the **linear relationship** between the predictors and the outcome\n",
    "- It can be influenced by the **number of predictors** in the model and may not always reflect the model's practical relevance or predictive power\n",
    "\n",
    "## 2. P-values: Evidence Against the Null Hypothesis\n",
    "**P-values** are used in hypothesis testing to determine whether there is a statistical evidence to reject the **null hypothesis** (usually assuming no effect). In this case, the coefficient's p-values are extremely small, indicating **strong evidence against the null hypothesis** of \"no effect\"\n",
    "\n",
    "## 3. Coefficients: Magnitude and Significance\n",
    "- **Coefficients** ($\\beta$) represent the **change in the outcome variable** for a one-unit change in the predictor, assuming all other predictors are held constant\n",
    "- For **categorical variables** (like Generation), the coefficient represents the **difference in the outcome** relative to a baseline category (e.g., \"Generation 1\")\n",
    "- **Large coefficients** with **strong evidence** (very small p-values) indicate that the predictors are significantly associated with the outcome, **after adjusting for other variables** in the model\n",
    "\n",
    "## 4. Addressing the Apparent Contradiction\n",
    "The apparent contradiction arises because **R-squared** and **p-values** (and their associated coefficients) measure different things:\n",
    "\n",
    "- **R-squared** reflects how much of the variability in the outcome is explained by the model. A low R-squared (like 17.6%) suggests that the model doesn't explain much of the variation in the outcome\n",
    "- **P-values** and **coefficients** assess the **statistical significance** of each predictor, regardless of how much variability is explained by the model. Even if a model explains only a small portion of the variability (low R-squared), some predictors might still have a **strong association** with the outcome, as indicated by their **large coefficients** and **small p-values**\n",
    "\n",
    "Thus, these results are not contradictory but rather reflect different aspects of the model:\n",
    "- **R-squared** tells how well the model fits the data as a whole\n",
    "- **P-values and coefficients** tell whether specific predictors are **statiscally significant** in influencing the outcome, regardless of the overall fit\n",
    "\n",
    "## 5. Interpreting Hypothesis Testing Results (p-values) and Model Fit (R-squared) Together\n",
    "They address different aspects of the model:\n",
    "- **R-squared** is about explaining the variation in the outcome\n",
    "- **p-values** (and coefficients) are about **testing whether a specific predictor is meaningful** in explaining the outcome\n",
    "\n",
    "A model can have **statistically significant predictors** (low p-values) with **low explanatory power** (low R-squared). This can occur when:\n",
    "- The predictors are **significant** but only have a **small effect** on the outcome\n",
    "- The model might **miss other important predictors**, which would explain the low R-squared\n",
    "\n",
    "## 6. Example of an Interpretation\n",
    "For the model formula HP ~ Q(\"Sp. Def\") * C(Generation), we are modeling the outcome HP with an interaction between a continuous variable (Sp. Def) and a categorical variable (Generation). \n",
    "\n",
    "- **Interaction term** (Q(\"Sp. Def\") * C(Generation)): the effect of Sp.Def on HP may vary by different Generation levels. If p-values for the interaction terms are very small, it means there's strong evidence that the effect of Sp. Def depends on the Generation\n",
    "- **Interpretation of large coefficients**: A large coefficient means that the effect of a variable (like Generation) on HP is substantial, but this doesn't necessarily mean that the model explains a large portion of the variance in HP. It may still be important to note that Generation or Sp. Def might influence HP significantly, even though the model only explains a small portion of the variation\n",
    "\n",
    "In sum, **R-squared** and **p-values** represent different types of information. They don't necessarily conflict but should be interpreted together to get a more comprehensive understanding of the model's fit and the significance of its predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2fccab",
   "metadata": {},
   "source": [
    "### **Summary of Interaction with ChatGPT:**\n",
    "\n",
    "As part of the data analysis for my assignment, I consulted with ChatGPT regarding the implementation of a logistic regression model to analyze relationships between certain variables in the Pokémon dataset.\n",
    "\n",
    "1. **Dataset and Model Setup:**\n",
    "   - I started with the Pokémon dataset, which contains various attributes like `HP`, `Sp. Def`, and `Generation`.\n",
    "   - The task was to analyze how the `Sp. Def` and `Generation` variables affect `HP`. Initially, I had issues with loading the dataset and specifying the correct model formula for interaction terms.\n",
    "\n",
    "2. **Code Troubleshooting:**\n",
    "   - The provided URL for the dataset was corrected to ensure proper loading.\n",
    "   - There was a clarification needed for specifying the interaction between the continuous variable `Sp. Def` and the categorical variable `Generation`. The correct formula for including this interaction term was explained.\n",
    "\n",
    "3. **Corrected Code:**\n",
    "   I received guidance to use the following code to load the dataset, fit the model, and display the results:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   import statsmodels.formula.api as smf\n",
    "\n",
    "   # Load the dataset\n",
    "   url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "   pokemon = pd.read_csv(url)\n",
    "\n",
    "   # Specify the model with interaction between Sp. Def and Generation\n",
    "   model_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokemon)\n",
    "\n",
    "   # Fit the model\n",
    "   model_fit = model_spec.fit()\n",
    "\n",
    "   # Display the summary of the model\n",
    "   print(model_fit.summary())\n",
    "   ```\n",
    "\n",
    "4. **Final Results:**\n",
    "   The code correctly fits an ordinary least squares (OLS) model that examines the interaction between `Sp. Def` (Special Defense) and `Generation` on `HP` (Hit Points). The output includes the regression coefficients, p-values, and R-squared value, which can be interpreted to understand the relationships between these variables.\n",
    "\n",
    "This interaction has been valuable in clarifying how to model and interpret the results of a regression with interaction terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cd064",
   "metadata": {},
   "source": [
    "# 5\n",
    "The five cells of code demonstrate the process of evaluating a regression model's generatlizability by comparing its \"in-sample\" and \"out-of-sample\" performance metrics, specifically through R-squared values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c213b6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>719</td>\n",
       "      <td>Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>719</td>\n",
       "      <td>DiancieMega Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Unbound</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Dark</td>\n",
       "      <td>80</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>721</td>\n",
       "      <td>Volcanion</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Water</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                   Name   Type 1  Type 2  HP  Attack  Defense  \\\n",
       "0      1              Bulbasaur    Grass  Poison  45      49       49   \n",
       "1      2                Ivysaur    Grass  Poison  60      62       63   \n",
       "2      3               Venusaur    Grass  Poison  80      82       83   \n",
       "3      3  VenusaurMega Venusaur    Grass  Poison  80     100      123   \n",
       "4      4             Charmander     Fire     NaN  39      52       43   \n",
       "..   ...                    ...      ...     ...  ..     ...      ...   \n",
       "795  719                Diancie     Rock   Fairy  50     100      150   \n",
       "796  719    DiancieMega Diancie     Rock   Fairy  50     160      110   \n",
       "797  720    HoopaHoopa Confined  Psychic   Ghost  80     110       60   \n",
       "798  720     HoopaHoopa Unbound  Psychic    Dark  80     160       60   \n",
       "799  721              Volcanion     Fire   Water  80     110      120   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0         65       65     45           1      False  \n",
       "1         80       80     60           1      False  \n",
       "2        100      100     80           1      False  \n",
       "3        122      120     80           1      False  \n",
       "4         60       50     65           1      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "795      100      150     50           6       True  \n",
       "796      160      110    110           6       True  \n",
       "797      150      130     70           6       True  \n",
       "798      170      130     80           6       True  \n",
       "799      130       90     70           6       True  \n",
       "\n",
       "[800 rows x 12 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url) \n",
    "pokeaman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6fd51ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:54:15</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n",
       "\\textbf{Date:}                           & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n",
       "\\textbf{Time:}                           &     02:54:15     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n",
       "\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n",
       "\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n",
       "\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n",
       "\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n",
       "\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n",
       "\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n",
       "\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n",
       "\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n",
       "\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.176\n",
       "Model:                            OLS   Adj. R-squared:                  0.164\n",
       "Method:                 Least Squares   F-statistic:                     15.27\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           3.50e-27\n",
       "Time:                        02:54:15   Log-Likelihood:                -3649.4\n",
       "No. Observations:                 800   AIC:                             7323.\n",
       "Df Residuals:                     788   BIC:                             7379.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
       "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
       "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
       "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
       "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
       "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
       "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
       "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
       "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
       "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
       "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
       "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
       "Skew:                           1.684   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman)\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb486386",
   "metadata": {},
   "source": [
    "## First Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "566524b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>338</td>\n",
       "      <td>Solrock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Charizard</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>109</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>224</td>\n",
       "      <td>Octillery</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>600</td>\n",
       "      <td>Klang</td>\n",
       "      <td>Steel</td>\n",
       "      <td>None</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>265</td>\n",
       "      <td>Wurmple</td>\n",
       "      <td>Bug</td>\n",
       "      <td>None</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>471</td>\n",
       "      <td>Glaceon</td>\n",
       "      <td>Ice</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>130</td>\n",
       "      <td>95</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>225</td>\n",
       "      <td>Delibird</td>\n",
       "      <td>Ice</td>\n",
       "      <td>Flying</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>109</td>\n",
       "      <td>Koffing</td>\n",
       "      <td>Poison</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>95</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>373</td>\n",
       "      <td>SalamenceMega Salamence</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>Flying</td>\n",
       "      <td>95</td>\n",
       "      <td>145</td>\n",
       "      <td>130</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                     Name   Type 1   Type 2  HP  Attack  Defense  \\\n",
       "370  338                  Solrock     Rock  Psychic  70      95       85   \n",
       "6      6                Charizard     Fire   Flying  78      84       78   \n",
       "242  224                Octillery    Water     None  75     105       75   \n",
       "661  600                    Klang    Steel     None  60      80       95   \n",
       "288  265                  Wurmple      Bug     None  45      45       35   \n",
       "..   ...                      ...      ...      ...  ..     ...      ...   \n",
       "522  471                  Glaceon      Ice     None  65      60      110   \n",
       "243  225                 Delibird      Ice   Flying  45      55       45   \n",
       "797  720      HoopaHoopa Confined  Psychic    Ghost  80     110       60   \n",
       "117  109                  Koffing   Poison     None  40      65       95   \n",
       "409  373  SalamenceMega Salamence   Dragon   Flying  95     145      130   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "370       55       65     70           3      False  \n",
       "6        109       85    100           1      False  \n",
       "242      105       75     45           2      False  \n",
       "661       70       85     50           5      False  \n",
       "288       20       30     20           3      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "522      130       95     65           4      False  \n",
       "243       65       45     75           2      False  \n",
       "797      150      130     70           6       True  \n",
       "117       60       45     35           1      False  \n",
       "409      120       90    120           3      False  \n",
       "\n",
       "[400 rows x 12 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c2b1c",
   "metadata": {},
   "source": [
    "## Code Explanation:\n",
    "#### 1. Data Splitting and Preprocessing:\n",
    "- The first code cell splits the pokemon dataset into two subsets: a training set (pokeaman_train) and a test set (pokeaman_test). The data is split in a 50-50 ratio, with a seed for reproducibility.\n",
    "- It also address any missing values in the \"Type 2\" column by filling them with \"None\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962381a1",
   "metadata": {},
   "source": [
    "## Second Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "78487eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.228109\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>str8fyre</td>     <th>  No. Observations:  </th>  <td>   800</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   788</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    11</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 15 Nov 2024</td> <th>  Pseudo R-squ.:     </th>  <td>0.05156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>02:54:15</td>     <th>  Log-Likelihood:    </th> <td> -182.49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -192.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.04757</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                <td>   -3.2644</td> <td>    0.714</td> <td>   -4.572</td> <td> 0.000</td> <td>   -4.664</td> <td>   -1.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                        <td>    4.3478</td> <td>    2.179</td> <td>    1.996</td> <td> 0.046</td> <td>    0.078</td> <td>    8.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 2\") == \"None\")[T.True]</th>         <td>    1.5432</td> <td>    0.853</td> <td>    1.810</td> <td> 0.070</td> <td>   -0.128</td> <td>    3.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>                       <td>   -0.0574</td> <td>    0.468</td> <td>   -0.123</td> <td> 0.902</td> <td>   -0.975</td> <td>    0.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>                       <td>   -0.6480</td> <td>    0.466</td> <td>   -1.390</td> <td> 0.164</td> <td>   -1.561</td> <td>    0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>                       <td>   -0.8255</td> <td>    0.545</td> <td>   -1.516</td> <td> 0.130</td> <td>   -1.893</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>                       <td>   -0.5375</td> <td>    0.449</td> <td>   -1.198</td> <td> 0.231</td> <td>   -1.417</td> <td>    0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>                       <td>    0.3213</td> <td>    0.477</td> <td>    0.673</td> <td> 0.501</td> <td>   -0.614</td> <td>    1.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                   <td>    0.0172</td> <td>    0.006</td> <td>    3.086</td> <td> 0.002</td> <td>    0.006</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                 <td>   -0.0365</td> <td>    0.019</td> <td>   -1.884</td> <td> 0.060</td> <td>   -0.074</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                  <td>   -0.0098</td> <td>    0.008</td> <td>   -1.247</td> <td> 0.213</td> <td>   -0.025</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:I(Q(\"Type 2\") == \"None\")[T.True]</th> <td>   -0.0197</td> <td>    0.012</td> <td>   -1.651</td> <td> 0.099</td> <td>   -0.043</td> <td>    0.004</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                           &     str8fyre     & \\textbf{  No. Observations:  } &      800    \\\\\n",
       "\\textbf{Model:}                                   &      Logit       & \\textbf{  Df Residuals:      } &      788    \\\\\n",
       "\\textbf{Method:}                                  &       MLE        & \\textbf{  Df Model:          } &       11    \\\\\n",
       "\\textbf{Date:}                                    & Fri, 15 Nov 2024 & \\textbf{  Pseudo R-squ.:     } &  0.05156    \\\\\n",
       "\\textbf{Time:}                                    &     02:54:15     & \\textbf{  Log-Likelihood:    } &   -182.49   \\\\\n",
       "\\textbf{converged:}                               &       True       & \\textbf{  LL-Null:           } &   -192.41   \\\\\n",
       "\\textbf{Covariance Type:}                         &    nonrobust     & \\textbf{  LLR p-value:       } &  0.04757    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                  & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                &      -3.2644  &        0.714     &    -4.572  &         0.000        &       -4.664    &       -1.865     \\\\\n",
       "\\textbf{Legendary[T.True]}                        &       4.3478  &        2.179     &     1.996  &         0.046        &        0.078    &        8.618     \\\\\n",
       "\\textbf{I(Q(\"Type 2\") == \"None\")[T.True]}         &       1.5432  &        0.853     &     1.810  &         0.070        &       -0.128    &        3.215     \\\\\n",
       "\\textbf{C(Generation)[T.2]}                       &      -0.0574  &        0.468     &    -0.123  &         0.902        &       -0.975    &        0.861     \\\\\n",
       "\\textbf{C(Generation)[T.3]}                       &      -0.6480  &        0.466     &    -1.390  &         0.164        &       -1.561    &        0.265     \\\\\n",
       "\\textbf{C(Generation)[T.4]}                       &      -0.8255  &        0.545     &    -1.516  &         0.130        &       -1.893    &        0.242     \\\\\n",
       "\\textbf{C(Generation)[T.5]}                       &      -0.5375  &        0.449     &    -1.198  &         0.231        &       -1.417    &        0.342     \\\\\n",
       "\\textbf{C(Generation)[T.6]}                       &       0.3213  &        0.477     &     0.673  &         0.501        &       -0.614    &        1.257     \\\\\n",
       "\\textbf{Attack}                                   &       0.0172  &        0.006     &     3.086  &         0.002        &        0.006    &        0.028     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                 &      -0.0365  &        0.019     &    -1.884  &         0.060        &       -0.074    &        0.001     \\\\\n",
       "\\textbf{Defense}                                  &      -0.0098  &        0.008     &    -1.247  &         0.213        &       -0.025    &        0.006     \\\\\n",
       "\\textbf{Defense:I(Q(\"Type 2\") == \"None\")[T.True]} &      -0.0197  &        0.012     &    -1.651  &         0.099        &       -0.043    &        0.004     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               str8fyre   No. Observations:                  800\n",
       "Model:                          Logit   Df Residuals:                      788\n",
       "Method:                           MLE   Df Model:                           11\n",
       "Date:                Fri, 15 Nov 2024   Pseudo R-squ.:                 0.05156\n",
       "Time:                        02:54:15   Log-Likelihood:                -182.49\n",
       "converged:                       True   LL-Null:                       -192.41\n",
       "Covariance Type:            nonrobust   LLR p-value:                   0.04757\n",
       "============================================================================================================\n",
       "                                               coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                   -3.2644      0.714     -4.572      0.000      -4.664      -1.865\n",
       "Legendary[T.True]                            4.3478      2.179      1.996      0.046       0.078       8.618\n",
       "I(Q(\"Type 2\") == \"None\")[T.True]             1.5432      0.853      1.810      0.070      -0.128       3.215\n",
       "C(Generation)[T.2]                          -0.0574      0.468     -0.123      0.902      -0.975       0.861\n",
       "C(Generation)[T.3]                          -0.6480      0.466     -1.390      0.164      -1.561       0.265\n",
       "C(Generation)[T.4]                          -0.8255      0.545     -1.516      0.130      -1.893       0.242\n",
       "C(Generation)[T.5]                          -0.5375      0.449     -1.198      0.231      -1.417       0.342\n",
       "C(Generation)[T.6]                           0.3213      0.477      0.673      0.501      -0.614       1.257\n",
       "Attack                                       0.0172      0.006      3.086      0.002       0.006       0.028\n",
       "Attack:Legendary[T.True]                    -0.0365      0.019     -1.884      0.060      -0.074       0.001\n",
       "Defense                                     -0.0098      0.008     -1.247      0.213      -0.025       0.006\n",
       "Defense:I(Q(\"Type 2\") == \"None\")[T.True]    -0.0197      0.012     -1.651      0.099      -0.043       0.004\n",
       "============================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1']=='Fire').astype(int)\n",
    "linear_model_specification_formula = \\\n",
    "'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=pokeaman).fit()\n",
    "log_reg_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd375c3",
   "metadata": {},
   "source": [
    "## Code Explanation:\n",
    "#### 2. Fitting a Basic Model on the Training Set:\n",
    "- In the second code cell, an OLS (ordinary least squares) regression model (model3_fit) is specified and fit using the Attack and Defense variables to predict HP.\n",
    "- By using only the training data, this model provides an \"in-sample\" understanding of the relationship between Attack, Defense, and HP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca34551",
   "metadata": {},
   "source": [
    "##  Third Code:\n",
    "(Had to add more codes to make sure that the third code given was not generating the NameError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9ce9496a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:54:16</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     02:54:16     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        02:54:16   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a5a2c531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n"
     ]
    }
   ],
   "source": [
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model3)[0, 1] ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c0bca3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n"
     ]
    }
   ],
   "source": [
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37deae33",
   "metadata": {},
   "source": [
    "## Code Explanation:\n",
    "#### 3. Evaluating \"In-Sample\" and \"Out-of-Sample\" R-squared:\n",
    "- The third code cell calculates the R-squared values for both \"in-sample\" and \"out-of-sample\" predictions.\n",
    "- The \"in-sample\" R-squared value (model3_fit.rsquared) measures how well the model fits the training data.\n",
    "- The \"out-of-sample\" R-squared is calculated as the sqaured correlation between the true HP values in the test set (y) and the model's predicted values (yhat_model3) for the test set. A lower \"out-of-sample\" R-squared relative to the \"in-sample\" R-squared indicates a lack of generalizability, suggesting overfitting in the difference is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c23d3b5",
   "metadata": {},
   "source": [
    "## Fourth Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6b2e1581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.23e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:54:17</td>     <th>  Log-Likelihood:    </th> <td> -1738.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3603.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   337</td>      <th>  BIC:               </th> <td>   3855.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    62</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                  <td></td>                                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                        <td>  521.5715</td> <td>  130.273</td> <td>    4.004</td> <td> 0.000</td> <td>  265.322</td> <td>  777.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                                                <td>   -6.1179</td> <td>    2.846</td> <td>   -2.150</td> <td> 0.032</td> <td>  -11.716</td> <td>   -0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                                           <td>   -8.1938</td> <td>    2.329</td> <td>   -3.518</td> <td> 0.000</td> <td>  -12.775</td> <td>   -3.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                                         <td>-1224.9610</td> <td>  545.105</td> <td>   -2.247</td> <td> 0.025</td> <td>-2297.199</td> <td> -152.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                                          <td>   -6.1989</td> <td>    2.174</td> <td>   -2.851</td> <td> 0.005</td> <td>  -10.475</td> <td>   -1.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]</th>                                        <td> -102.4030</td> <td>   96.565</td> <td>   -1.060</td> <td> 0.290</td> <td> -292.350</td> <td>   87.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense</th>                                                   <td>    0.0985</td> <td>    0.033</td> <td>    2.982</td> <td> 0.003</td> <td>    0.034</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]</th>                                 <td>   14.6361</td> <td>    6.267</td> <td>    2.336</td> <td> 0.020</td> <td>    2.310</td> <td>   26.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                                            <td>   -7.2261</td> <td>    2.178</td> <td>   -3.318</td> <td> 0.001</td> <td>  -11.511</td> <td>   -2.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]</th>                                          <td>  704.8798</td> <td>  337.855</td> <td>    2.086</td> <td> 0.038</td> <td>   40.309</td> <td> 1369.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                                                     <td>    0.1264</td> <td>    0.038</td> <td>    3.351</td> <td> 0.001</td> <td>    0.052</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]</th>                                   <td>    5.8648</td> <td>    2.692</td> <td>    2.179</td> <td> 0.030</td> <td>    0.570</td> <td>   11.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed</th>                                                    <td>    0.1026</td> <td>    0.039</td> <td>    2.634</td> <td> 0.009</td> <td>    0.026</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]</th>                                  <td>   -6.9266</td> <td>    3.465</td> <td>   -1.999</td> <td> 0.046</td> <td>  -13.742</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed</th>                                             <td>   -0.0016</td> <td>    0.001</td> <td>   -2.837</td> <td> 0.005</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]</th>                           <td>   -0.0743</td> <td>    0.030</td> <td>   -2.477</td> <td> 0.014</td> <td>   -0.133</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                                                     <td>   -5.3982</td> <td>    1.938</td> <td>   -2.785</td> <td> 0.006</td> <td>   -9.211</td> <td>   -1.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\")</th>                                   <td> -282.2496</td> <td>  126.835</td> <td>   -2.225</td> <td> 0.027</td> <td> -531.738</td> <td>  -32.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                                              <td>    0.1094</td> <td>    0.034</td> <td>    3.233</td> <td> 0.001</td> <td>    0.043</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\")</th>                            <td>   12.6503</td> <td>    5.851</td> <td>    2.162</td> <td> 0.031</td> <td>    1.141</td> <td>   24.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\")</th>                                             <td>    0.0628</td> <td>    0.028</td> <td>    2.247</td> <td> 0.025</td> <td>    0.008</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                           <td>    3.3949</td> <td>    1.783</td> <td>    1.904</td> <td> 0.058</td> <td>   -0.112</td> <td>    6.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\")</th>                                      <td>   -0.0012</td> <td>    0.000</td> <td>   -2.730</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                    <td>   -0.1456</td> <td>    0.065</td> <td>   -2.253</td> <td> 0.025</td> <td>   -0.273</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                                               <td>    0.0624</td> <td>    0.031</td> <td>    2.027</td> <td> 0.043</td> <td>    0.002</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                             <td>   -3.2219</td> <td>    1.983</td> <td>   -1.625</td> <td> 0.105</td> <td>   -7.122</td> <td>    0.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>                                        <td>   -0.0014</td> <td>    0.001</td> <td>   -2.732</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                      <td>   -0.0695</td> <td>    0.033</td> <td>   -2.100</td> <td> 0.036</td> <td>   -0.135</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\")</th>                                       <td>   -0.0008</td> <td>    0.000</td> <td>   -1.743</td> <td> 0.082</td> <td>   -0.002</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                     <td>    0.0334</td> <td>    0.021</td> <td>    1.569</td> <td> 0.117</td> <td>   -0.008</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\")</th>                                <td> 1.629e-05</td> <td> 6.92e-06</td> <td>    2.355</td> <td> 0.019</td> <td> 2.68e-06</td> <td> 2.99e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>              <td>    0.0008</td> <td>    0.000</td> <td>    2.433</td> <td> 0.015</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                                                     <td>   -8.3636</td> <td>    2.346</td> <td>   -3.565</td> <td> 0.000</td> <td>  -12.978</td> <td>   -3.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Atk\")</th>                                   <td>  850.5436</td> <td>  385.064</td> <td>    2.209</td> <td> 0.028</td> <td>   93.112</td> <td> 1607.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                                              <td>    0.1388</td> <td>    0.040</td> <td>    3.500</td> <td> 0.001</td> <td>    0.061</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Atk\")</th>                            <td>    2.1809</td> <td>    1.136</td> <td>    1.920</td> <td> 0.056</td> <td>   -0.054</td> <td>    4.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Atk\")</th>                                             <td>    0.0831</td> <td>    0.038</td> <td>    2.162</td> <td> 0.031</td> <td>    0.007</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                           <td>   -7.3121</td> <td>    3.376</td> <td>   -2.166</td> <td> 0.031</td> <td>  -13.953</td> <td>   -0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Atk\")</th>                                      <td>   -0.0014</td> <td>    0.001</td> <td>   -2.480</td> <td> 0.014</td> <td>   -0.003</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                    <td>   -0.0434</td> <td>    0.022</td> <td>   -2.010</td> <td> 0.045</td> <td>   -0.086</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                                               <td>    0.1011</td> <td>    0.035</td> <td>    2.872</td> <td> 0.004</td> <td>    0.032</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                             <td>  -12.6343</td> <td>    5.613</td> <td>   -2.251</td> <td> 0.025</td> <td>  -23.674</td> <td>   -1.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>                                        <td>   -0.0018</td> <td>    0.001</td> <td>   -3.102</td> <td> 0.002</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                      <td>    0.0151</td> <td>    0.009</td> <td>    1.609</td> <td> 0.109</td> <td>   -0.003</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Atk\")</th>                                       <td>   -0.0012</td> <td>    0.001</td> <td>   -1.860</td> <td> 0.064</td> <td>   -0.002</td> <td> 6.62e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                     <td>    0.1210</td> <td>    0.054</td> <td>    2.260</td> <td> 0.024</td> <td>    0.016</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Atk\")</th>                                <td> 2.125e-05</td> <td>  9.1e-06</td> <td>    2.334</td> <td> 0.020</td> <td> 3.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>              <td> 6.438e-06</td> <td> 7.69e-05</td> <td>    0.084</td> <td> 0.933</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                        <td>    0.1265</td> <td>    0.033</td> <td>    3.821</td> <td> 0.000</td> <td>    0.061</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                      <td>   -5.0544</td> <td>    2.506</td> <td>   -2.017</td> <td> 0.044</td> <td>   -9.983</td> <td>   -0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                 <td>   -0.0021</td> <td>    0.001</td> <td>   -3.606</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>               <td>   -0.0346</td> <td>    0.017</td> <td>   -1.992</td> <td> 0.047</td> <td>   -0.069</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                <td>   -0.0012</td> <td>    0.000</td> <td>   -2.406</td> <td> 0.017</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0446</td> <td>    0.025</td> <td>    1.794</td> <td> 0.074</td> <td>   -0.004</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                         <td> 1.973e-05</td> <td> 7.28e-06</td> <td>    2.710</td> <td> 0.007</td> <td> 5.41e-06</td> <td>  3.4e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>    0.0005</td> <td>    0.000</td> <td>    1.957</td> <td> 0.051</td> <td>-2.56e-06</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                  <td>   -0.0013</td> <td>    0.000</td> <td>   -2.740</td> <td> 0.006</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                <td>    0.0841</td> <td>    0.040</td> <td>    2.125</td> <td> 0.034</td> <td>    0.006</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                           <td> 2.379e-05</td> <td> 7.85e-06</td> <td>    3.030</td> <td> 0.003</td> <td> 8.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>         <td> 2.864e-05</td> <td> 7.73e-05</td> <td>    0.370</td> <td> 0.711</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                          <td> 1.284e-05</td> <td> 7.46e-06</td> <td>    1.721</td> <td> 0.086</td> <td>-1.83e-06</td> <td> 2.75e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0008</td> <td>    0.000</td> <td>   -2.085</td> <td> 0.038</td> <td>   -0.002</td> <td>-4.68e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                   <td> -2.53e-07</td> <td>  1.1e-07</td> <td>   -2.292</td> <td> 0.023</td> <td> -4.7e-07</td> <td>-3.59e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>-1.425e-06</td> <td> 1.14e-06</td> <td>   -1.249</td> <td> 0.212</td> <td>-3.67e-06</td> <td> 8.19e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.2e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                                                   &        HP        & \\textbf{  R-squared:         } &     0.467   \\\\\n",
       "\\textbf{Model:}                                                           &       OLS        & \\textbf{  Adj. R-squared:    } &     0.369   \\\\\n",
       "\\textbf{Method:}                                                          &  Least Squares   & \\textbf{  F-statistic:       } &     4.764   \\\\\n",
       "\\textbf{Date:}                                                            & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.23e-21   \\\\\n",
       "\\textbf{Time:}                                                            &     02:54:17     & \\textbf{  Log-Likelihood:    } &   -1738.6   \\\\\n",
       "\\textbf{No. Observations:}                                                &         400      & \\textbf{  AIC:               } &     3603.   \\\\\n",
       "\\textbf{Df Residuals:}                                                    &         337      & \\textbf{  BIC:               } &     3855.   \\\\\n",
       "\\textbf{Df Model:}                                                        &          62      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                                                 &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                                        &     521.5715  &      130.273     &     4.004  &         0.000        &      265.322    &      777.821     \\\\\n",
       "\\textbf{Legendary[T.True]}                                                &      -6.1179  &        2.846     &    -2.150  &         0.032        &      -11.716    &       -0.520     \\\\\n",
       "\\textbf{Attack}                                                           &      -8.1938  &        2.329     &    -3.518  &         0.000        &      -12.775    &       -3.612     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                                         &   -1224.9610  &      545.105     &    -2.247  &         0.025        &    -2297.199    &     -152.723     \\\\\n",
       "\\textbf{Defense}                                                          &      -6.1989  &        2.174     &    -2.851  &         0.005        &      -10.475    &       -1.923     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]}                                        &    -102.4030  &       96.565     &    -1.060  &         0.290        &     -292.350    &       87.544     \\\\\n",
       "\\textbf{Attack:Defense}                                                   &       0.0985  &        0.033     &     2.982  &         0.003        &        0.034    &        0.164     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]}                                 &      14.6361  &        6.267     &     2.336  &         0.020        &        2.310    &       26.963     \\\\\n",
       "\\textbf{Speed}                                                            &      -7.2261  &        2.178     &    -3.318  &         0.001        &      -11.511    &       -2.942     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]}                                          &     704.8798  &      337.855     &     2.086  &         0.038        &       40.309    &     1369.450     \\\\\n",
       "\\textbf{Attack:Speed}                                                     &       0.1264  &        0.038     &     3.351  &         0.001        &        0.052    &        0.201     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]}                                   &       5.8648  &        2.692     &     2.179  &         0.030        &        0.570    &       11.160     \\\\\n",
       "\\textbf{Defense:Speed}                                                    &       0.1026  &        0.039     &     2.634  &         0.009        &        0.026    &        0.179     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]}                                  &      -6.9266  &        3.465     &    -1.999  &         0.046        &      -13.742    &       -0.111     \\\\\n",
       "\\textbf{Attack:Defense:Speed}                                             &      -0.0016  &        0.001     &    -2.837  &         0.005        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]}                           &      -0.0743  &        0.030     &    -2.477  &         0.014        &       -0.133    &       -0.015     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                                                     &      -5.3982  &        1.938     &    -2.785  &         0.006        &       -9.211    &       -1.586     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\")}                                   &    -282.2496  &      126.835     &    -2.225  &         0.027        &     -531.738    &      -32.761     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                                              &       0.1094  &        0.034     &     3.233  &         0.001        &        0.043    &        0.176     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\")}                            &      12.6503  &        5.851     &     2.162  &         0.031        &        1.141    &       24.160     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\")}                                             &       0.0628  &        0.028     &     2.247  &         0.025        &        0.008    &        0.118     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\")}                           &       3.3949  &        1.783     &     1.904  &         0.058        &       -0.112    &        6.902     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\")}                                      &      -0.0012  &        0.000     &    -2.730  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")}                    &      -0.1456  &        0.065     &    -2.253  &         0.025        &       -0.273    &       -0.018     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                                               &       0.0624  &        0.031     &     2.027  &         0.043        &        0.002    &        0.123     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\")}                             &      -3.2219  &        1.983     &    -1.625  &         0.105        &       -7.122    &        0.678     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}                                        &      -0.0014  &        0.001     &    -2.732  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                      &      -0.0695  &        0.033     &    -2.100  &         0.036        &       -0.135    &       -0.004     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\")}                                       &      -0.0008  &        0.000     &    -1.743  &         0.082        &       -0.002    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                     &       0.0334  &        0.021     &     1.569  &         0.117        &       -0.008    &        0.075     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\")}                                &    1.629e-05  &     6.92e-06     &     2.355  &         0.019        &     2.68e-06    &     2.99e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}              &       0.0008  &        0.000     &     2.433  &         0.015        &        0.000    &        0.001     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                                                     &      -8.3636  &        2.346     &    -3.565  &         0.000        &      -12.978    &       -3.749     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Atk\")}                                   &     850.5436  &      385.064     &     2.209  &         0.028        &       93.112    &     1607.975     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                                              &       0.1388  &        0.040     &     3.500  &         0.001        &        0.061    &        0.217     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Atk\")}                            &       2.1809  &        1.136     &     1.920  &         0.056        &       -0.054    &        4.416     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Atk\")}                                             &       0.0831  &        0.038     &     2.162  &         0.031        &        0.007    &        0.159     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                           &      -7.3121  &        3.376     &    -2.166  &         0.031        &      -13.953    &       -0.671     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Atk\")}                                      &      -0.0014  &        0.001     &    -2.480  &         0.014        &       -0.003    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                    &      -0.0434  &        0.022     &    -2.010  &         0.045        &       -0.086    &       -0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                                               &       0.1011  &        0.035     &     2.872  &         0.004        &        0.032    &        0.170     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                             &     -12.6343  &        5.613     &    -2.251  &         0.025        &      -23.674    &       -1.594     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}                                        &      -0.0018  &        0.001     &    -3.102  &         0.002        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                      &       0.0151  &        0.009     &     1.609  &         0.109        &       -0.003    &        0.034     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Atk\")}                                       &      -0.0012  &        0.001     &    -1.860  &         0.064        &       -0.002    &     6.62e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                     &       0.1210  &        0.054     &     2.260  &         0.024        &        0.016    &        0.226     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Atk\")}                                &    2.125e-05  &      9.1e-06     &     2.334  &         0.020        &     3.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}              &    6.438e-06  &     7.69e-05     &     0.084  &         0.933        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                        &       0.1265  &        0.033     &     3.821  &         0.000        &        0.061    &        0.192     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                      &      -5.0544  &        2.506     &    -2.017  &         0.044        &       -9.983    &       -0.126     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                 &      -0.0021  &        0.001     &    -3.606  &         0.000        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}               &      -0.0346  &        0.017     &    -1.992  &         0.047        &       -0.069    &       -0.000     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                &      -0.0012  &        0.000     &    -2.406  &         0.017        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0446  &        0.025     &     1.794  &         0.074        &       -0.004    &        0.093     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                         &    1.973e-05  &     7.28e-06     &     2.710  &         0.007        &     5.41e-06    &      3.4e-05     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &       0.0005  &        0.000     &     1.957  &         0.051        &    -2.56e-06    &        0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                  &      -0.0013  &        0.000     &    -2.740  &         0.006        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                &       0.0841  &        0.040     &     2.125  &         0.034        &        0.006    &        0.162     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                           &    2.379e-05  &     7.85e-06     &     3.030  &         0.003        &     8.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}         &    2.864e-05  &     7.73e-05     &     0.370  &         0.711        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                          &    1.284e-05  &     7.46e-06     &     1.721  &         0.086        &    -1.83e-06    &     2.75e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0008  &        0.000     &    -2.085  &         0.038        &       -0.002    &    -4.68e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                   &    -2.53e-07  &      1.1e-07     &    -2.292  &         0.023        &     -4.7e-07    &    -3.59e-08     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &   -1.425e-06  &     1.14e-06     &    -1.249  &         0.212        &    -3.67e-06    &     8.19e-07     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.2e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.467\n",
       "Model:                            OLS   Adj. R-squared:                  0.369\n",
       "Method:                 Least Squares   F-statistic:                     4.764\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           4.23e-21\n",
       "Time:                        02:54:17   Log-Likelihood:                -1738.6\n",
       "No. Observations:                 400   AIC:                             3603.\n",
       "Df Residuals:                     337   BIC:                             3855.\n",
       "Df Model:                          62                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================================================================\n",
       "                                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                          521.5715    130.273      4.004      0.000     265.322     777.821\n",
       "Legendary[T.True]                                                   -6.1179      2.846     -2.150      0.032     -11.716      -0.520\n",
       "Attack                                                              -8.1938      2.329     -3.518      0.000     -12.775      -3.612\n",
       "Attack:Legendary[T.True]                                         -1224.9610    545.105     -2.247      0.025   -2297.199    -152.723\n",
       "Defense                                                             -6.1989      2.174     -2.851      0.005     -10.475      -1.923\n",
       "Defense:Legendary[T.True]                                         -102.4030     96.565     -1.060      0.290    -292.350      87.544\n",
       "Attack:Defense                                                       0.0985      0.033      2.982      0.003       0.034       0.164\n",
       "Attack:Defense:Legendary[T.True]                                    14.6361      6.267      2.336      0.020       2.310      26.963\n",
       "Speed                                                               -7.2261      2.178     -3.318      0.001     -11.511      -2.942\n",
       "Speed:Legendary[T.True]                                            704.8798    337.855      2.086      0.038      40.309    1369.450\n",
       "Attack:Speed                                                         0.1264      0.038      3.351      0.001       0.052       0.201\n",
       "Attack:Speed:Legendary[T.True]                                       5.8648      2.692      2.179      0.030       0.570      11.160\n",
       "Defense:Speed                                                        0.1026      0.039      2.634      0.009       0.026       0.179\n",
       "Defense:Speed:Legendary[T.True]                                     -6.9266      3.465     -1.999      0.046     -13.742      -0.111\n",
       "Attack:Defense:Speed                                                -0.0016      0.001     -2.837      0.005      -0.003      -0.001\n",
       "Attack:Defense:Speed:Legendary[T.True]                              -0.0743      0.030     -2.477      0.014      -0.133      -0.015\n",
       "Q(\"Sp. Def\")                                                        -5.3982      1.938     -2.785      0.006      -9.211      -1.586\n",
       "Legendary[T.True]:Q(\"Sp. Def\")                                    -282.2496    126.835     -2.225      0.027    -531.738     -32.761\n",
       "Attack:Q(\"Sp. Def\")                                                  0.1094      0.034      3.233      0.001       0.043       0.176\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\")                               12.6503      5.851      2.162      0.031       1.141      24.160\n",
       "Defense:Q(\"Sp. Def\")                                                 0.0628      0.028      2.247      0.025       0.008       0.118\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\")                               3.3949      1.783      1.904      0.058      -0.112       6.902\n",
       "Attack:Defense:Q(\"Sp. Def\")                                         -0.0012      0.000     -2.730      0.007      -0.002      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")                       -0.1456      0.065     -2.253      0.025      -0.273      -0.018\n",
       "Speed:Q(\"Sp. Def\")                                                   0.0624      0.031      2.027      0.043       0.002       0.123\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\")                                -3.2219      1.983     -1.625      0.105      -7.122       0.678\n",
       "Attack:Speed:Q(\"Sp. Def\")                                           -0.0014      0.001     -2.732      0.007      -0.002      -0.000\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         -0.0695      0.033     -2.100      0.036      -0.135      -0.004\n",
       "Defense:Speed:Q(\"Sp. Def\")                                          -0.0008      0.000     -1.743      0.082      -0.002       0.000\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         0.0334      0.021      1.569      0.117      -0.008       0.075\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\")                                 1.629e-05   6.92e-06      2.355      0.019    2.68e-06    2.99e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                  0.0008      0.000      2.433      0.015       0.000       0.001\n",
       "Q(\"Sp. Atk\")                                                        -8.3636      2.346     -3.565      0.000     -12.978      -3.749\n",
       "Legendary[T.True]:Q(\"Sp. Atk\")                                     850.5436    385.064      2.209      0.028      93.112    1607.975\n",
       "Attack:Q(\"Sp. Atk\")                                                  0.1388      0.040      3.500      0.001       0.061       0.217\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Atk\")                                2.1809      1.136      1.920      0.056      -0.054       4.416\n",
       "Defense:Q(\"Sp. Atk\")                                                 0.0831      0.038      2.162      0.031       0.007       0.159\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Atk\")                              -7.3121      3.376     -2.166      0.031     -13.953      -0.671\n",
       "Attack:Defense:Q(\"Sp. Atk\")                                         -0.0014      0.001     -2.480      0.014      -0.003      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")                       -0.0434      0.022     -2.010      0.045      -0.086      -0.001\n",
       "Speed:Q(\"Sp. Atk\")                                                   0.1011      0.035      2.872      0.004       0.032       0.170\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Atk\")                               -12.6343      5.613     -2.251      0.025     -23.674      -1.594\n",
       "Attack:Speed:Q(\"Sp. Atk\")                                           -0.0018      0.001     -3.102      0.002      -0.003      -0.001\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                          0.0151      0.009      1.609      0.109      -0.003       0.034\n",
       "Defense:Speed:Q(\"Sp. Atk\")                                          -0.0012      0.001     -1.860      0.064      -0.002    6.62e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                         0.1210      0.054      2.260      0.024       0.016       0.226\n",
       "Attack:Defense:Speed:Q(\"Sp. Atk\")                                 2.125e-05    9.1e-06      2.334      0.020    3.34e-06    3.92e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")               6.438e-06   7.69e-05      0.084      0.933      -0.000       0.000\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                            0.1265      0.033      3.821      0.000       0.061       0.192\n",
       "Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                         -5.0544      2.506     -2.017      0.044      -9.983      -0.126\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                    -0.0021      0.001     -3.606      0.000      -0.003      -0.001\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  -0.0346      0.017     -1.992      0.047      -0.069      -0.000\n",
       "Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                   -0.0012      0.000     -2.406      0.017      -0.002      -0.000\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0446      0.025      1.794      0.074      -0.004       0.093\n",
       "Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                          1.973e-05   7.28e-06      2.710      0.007    5.41e-06     3.4e-05\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           0.0005      0.000      1.957      0.051   -2.56e-06       0.001\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                     -0.0013      0.000     -2.740      0.006      -0.002      -0.000\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    0.0841      0.040      2.125      0.034       0.006       0.162\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                            2.379e-05   7.85e-06      3.030      0.003    8.34e-06    3.92e-05\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          2.864e-05   7.73e-05      0.370      0.711      -0.000       0.000\n",
       "Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                           1.284e-05   7.46e-06      1.721      0.086   -1.83e-06    2.75e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0008      0.000     -2.085      0.038      -0.002   -4.68e-05\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    -2.53e-07    1.1e-07     -2.292      0.023    -4.7e-07   -3.59e-08\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\") -1.425e-06   1.14e-06     -1.249      0.212   -3.67e-06    8.19e-07\n",
       "==============================================================================\n",
       "Omnibus:                      214.307   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2354.664\n",
       "Skew:                           2.026   Prob(JB):                         0.00\n",
       "Kurtosis:                      14.174   Cond. No.                     1.20e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.2e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1139a1",
   "metadata": {},
   "source": [
    "## Code Explanation:\n",
    "#### 4. Building a Complex Model with Interactions:\n",
    "- The fourth code cell specifies a more complex with multiple interaction terms, including variables such as Attack, Defense, Speed, Legendary, Sp. Def, and Sp. Atk. This model explores how various predictors and their interactions influence HP.\n",
    "- While such complexity could improve the fit for training data, it increases the risk of overfitting, where the model captures noise rather than general patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086ae096",
   "metadata": {},
   "source": [
    "## Fifth Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bd8eee0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.46709442115833855\n",
      "'Out of sample' R-squared: 0.002485342598992873\n"
     ]
    }
   ],
   "source": [
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaac7b7",
   "metadata": {},
   "source": [
    "## Code Explanation:\n",
    "#### 5. Evaluating the Complex Model:\n",
    "- The fifth code cell involve calculating both \"in-sample\" and \"out-of-sample\" R-squared values for this complex model. This comparison shows how well the model generalizes to new data despite its high complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39aec9",
   "metadata": {},
   "source": [
    "## Illustration of Concepts:\n",
    "- **R-squared as Measure of Fit and Generalizability**: \n",
    "    - The code illustrates how R-squared values (calculated as the squared correlation) assess model fit. Higher in-sample R-squared values reflect better fit on the training data, but when out-of-sample R-squared drops, it suggests overfitting.\n",
    "    \n",
    "- **In-Sample vs. Out-of-Sample Performance**:\n",
    "    - Comparing these two metrics allows us to evaluate the model's generalizability. If the complex model's out-of-sample R-squared is much lower than its in-sample R-squared, the model likely overfits the training data, making it less reliable for predicting new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638715a9",
   "metadata": {},
   "source": [
    "# ChatGPT Summary:\n",
    "**Interaction Summary:**\n",
    "\n",
    "1. **Discussion of Code and Model Interpretation:**\n",
    "   You shared multiple pieces of code related to your analysis and discussed their purpose in evaluating model performance.\n",
    "\n",
    "   - The first code involved splitting a dataset (`pokeaman`) into a training and testing set, filling missing values, and using the `train_test_split` function from `sklearn`.\n",
    "   - The second piece of code defined a linear regression model using `statsmodels` to predict a target variable (`HP`) based on predictors (`Attack` and `Defense`) and fit the model on the training set.\n",
    "   - The third block calculated \"in-sample\" R-squared (using the model's `rsquared` attribute) and \"out-of-sample\" R-squared (using `np.corrcoef` between actual and predicted values from the test set).\n",
    "   - In the fourth piece of code, a more complex model was specified using multiple predictors with interaction terms (including categorical variables). It was observed that this model was computationally intensive and potentially could lead to overfitting, especially with a large number of interactions.\n",
    "\n",
    "2. **Guidance on Model Interpretation:**\n",
    "   You received clarification on interpreting R-squared values:\n",
    "   - **In-sample R-squared** measures the model's ability to explain the variability in the training data.\n",
    "   - **Out-of-sample R-squared** gauges how well the model generalizes to unseen data. A significant drop between the two can indicate overfitting.\n",
    "   - This distinction is essential for evaluating model performance and ensuring that the model is not simply memorizing the training data but is able to generalize to new data.\n",
    "\n",
    "3. **Error Handling and Next Steps:**\n",
    "   - You encountered a `NameError` due to running the model's prediction before properly defining it. This was fixed by ensuring the model was correctly fitted before making predictions.\n",
    "   - You were advised on how to run and debug the code on your JupyterHub environment and ensure all steps were properly executed in sequence to avoid errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbae91b",
   "metadata": {},
   "source": [
    "# 7\n",
    "\n",
    "## Explanation of Multicollinearity and Its Impact on Model Generalizability:\n",
    "\n",
    "#### 1. Design Matrix and Predictor Variables:\n",
    "The \"design matrix\" of a regression model is a matrix of predictor variables (explanatory variables) used to preict the outcome variable. In the case of **model 4**, the variable `model4_spec.exog` represents the design matrix used in the model. This matrix contains columns of the predictors, and the model uses these predictors to fit the regression model and estimate the coefficients. The shape of the design matrix (`model4_spec.exog.shape`) indicates the number of observations and the number of predictors (including any interaction terms).\n",
    "\n",
    "#### 2. Multicollinearity in the Design Matrix:\n",
    "Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated. This correlation makes it difficult to isolate the individual effect of each predictor on the outcome variable because the predictors \"overlap\" in the information they provide. In the case of **model4**, which includes a large number of interaction terms, multicollinearity is likely to be a significant issue. The more predictors and interactions are included, the more likely it is that these predictors will be correlated, causing multicollinearity. \n",
    "\n",
    "This multicollinearity can be detected using the condition number of the design matrix. The condition number is a measure of the matrix's sensitivity to small changes in the data. A very large condition number indicates the presence of significant multicollinearity, which suggests that the model might be unstable and that its predictions may not generalize well to new data. **Model 4** has a very large condition number, indicating a high degree of multicollinearity.\n",
    "\n",
    "#### 3. Impact of Multicollinearity on Genearlizability:\n",
    "When predictor variables are highly correlated, it becomes difficult for the model to reliably estimate the contribution of each individual predictor. This results in overfitting, where the model fits the training data too closely, capturing noise and idiosyncrasies that don't generalize to the testing set. In other words, while the model might perform well on the training data, it struggles to make accurate predictions on new data because it has essentially \"memorized\" the training data rather than learning the underlying patterns. \n",
    "\n",
    "In contrast, a simpler model like **model3**, which uses fewer predictors, is less prone to overfitting because it avoids the complex relationships introduced by too many correlated predictors. This model is more likely to identify real, generalizable patterns that persist across both training and testing datasets. \n",
    "\n",
    "#### 4. Centering and Scaling of Variables:\n",
    "**Centering and scaling** predictor variables is a standard practice in regression analysis to improve model stability and to facilitate the interpretation of multicollinearity. This process transforms variables by subtracting their mean (centering) and dividing by their standard deviation (scaling). This helps in making sure that all predictor variables are on the same scale, which is crucial for detecting and quantifying multicollinearity. \n",
    "\n",
    "The condition number of a model can be artificially inflated if centering and scaling are not applied. For example, without centering and scaling, **model4** reports a very high condition number, suggesting severe multicollinearity. However, after centering and scaling, the condition number remains very large, reinforcing the idea that **model4** is highly multicollinear and overfitted.\n",
    "\n",
    "#### 5. Conclusion:\n",
    "The presence of multicollinearity in the design matrix of **model4** leads to overfitting, where the model fits the training data too closely and fails to generalize well to new data. This issue is exacerbated by the large number of predictors and interaction terms, which introduces complexity beyond the capacity of the data. The high condition number serves as a diagnostic tool to reveal the level of multicollinearity and provides a warning that the model's generalizability is likely compromised. In contrast, simpler models with fewer predictors are less prone to overfitting and tend to perform better on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042bafac",
   "metadata": {},
   "source": [
    "# ChatGPT Summary:\n",
    "### **Summary of Interaction**\n",
    "\n",
    "1. **Data Splitting:**\n",
    "   You split the dataset into training and testing sets, using a 50/50 ratio, and ensured that missing values in the \"Type 2\" column were replaced with 'None'. This splitting enabled you to calculate \"in-sample\" R-squared using the training dataset and \"out-of-sample\" R-squared using the testing dataset.\n",
    "\n",
    "2. **Model 3 (Simpler Model):**\n",
    "   You fitted a linear regression model using a formula with two predictors: `Attack` and `Defense`. The \"in-sample\" R-squared value was calculated directly from the model fit, and \"out-of-sample\" R-squared was computed based on predictions from the test set. The results indicated that this simpler model provided good generalization from the training data to the testing data.\n",
    "\n",
    "3. **Model 4 (Complex Model):**\n",
    "   You then created a much more complex model involving a large number of predictor variables, including interactions among several features. The model was fit, but it resulted in overfitting, where the model identified patterns that were specific to the training set but did not generalize well to the test data. This overfitting was indicated by a sharp drop in \"out-of-sample\" R-squared compared to \"in-sample\" R-squared.\n",
    "\n",
    "4. **Multicollinearity and Generalization:**\n",
    "   You also explored the concept of multicollinearity in the design matrix. It was observed that the complexity of the predictors in the model led to a very large condition number, indicating multicollinearity. This suggested that the model's generalizability was compromised because highly correlated predictors created issues in the model's ability to generalize to new, unseen data.\n",
    "\n",
    "5. **Tools and Diagnostics:**\n",
    "   The issue of multicollinearity was diagnosed using the condition number of the \"design matrix,\" and you also discussed the potential utility of centering and scaling the predictor variables to better understand the degree of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da613aa8",
   "metadata": {},
   "source": [
    "# 8\n",
    "\n",
    "## 1. Code for Looping Over Model Performance Metrics:\n",
    "- Split the Pokemon data repetedly, using a linear regression model on each split without setting a random seed, to create different training and testing sets each time. This allows to collect a range of \"in-sample\" and \"out-of-sample\" R-squared values and visualize them.\n",
    "\n",
    "## 2. Explanation of Results and Purpose:\n",
    "- The purpose of repeatedly splitting and testing the model is to observe how performance metrics fluctuate due to the random variation in the data splits. The difference between \"in-sample\" and \"out-of-sample\" R-squared values can reveal the potential instability and variance in model performance, indicating sensitivity to overfitting or multicollinearity, especially if the \"out-of-sample\" performance is consistently lower than \"in-sample.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "98ce8905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "In Sample Performance (R-squared)=%{x}<br>Out of Sample Performance (R-squared)=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.1678367784811714,
          0.11063387541129288,
          0.2180252435846015,
          0.2247685059961666,
          0.1930648587344811,
          0.08736776568684879,
          0.23288670034407188,
          0.11838142587409073,
          0.21932073090607662,
          0.23395968115467036,
          0.18372275075966804,
          0.31027605307729755,
          0.17446272568439936,
          0.19454495605237732,
          0.20091261610892486,
          0.26171058427318694,
          0.21353066204339566,
          0.15303386396357765,
          0.2252228876996677,
          0.15264261160233272,
          0.21455739745509594,
          0.3131236042002644,
          0.11163091513197654,
          0.1954061237698429,
          0.11945135996017253,
          0.2286780432439801,
          0.1732312337122771,
          0.1058249717287656,
          0.16885281804993102,
          0.16181430196730628,
          0.1953575872715918,
          0.17491554267906861,
          0.1054652563081736,
          0.2919347991723632,
          0.19898400701944596,
          0.19317604994505944,
          0.12150909379820063,
          0.17241361286553225,
          0.10024983253580655,
          0.2033775584879346,
          0.1806803714163251,
          0.2703559187986564,
          0.17050873655238252,
          0.17148768769584422,
          0.16655780525172026,
          0.22634742861243717,
          0.21136421183179366,
          0.20670943192350366,
          0.18716718321093018,
          0.26324712646007764,
          0.18320225374344212,
          0.23694459428466652,
          0.25630615717920613,
          0.2716887637087,
          0.18205482280932384,
          0.1417595388461531,
          0.24198061060384424,
          0.17648764597980138,
          0.1496684828877648,
          0.2006304701237559,
          0.22886425932250987,
          0.15381251264049933,
          0.25389134510647726,
          0.11792735972745061,
          0.22429211012039973,
          0.2939251438246523,
          0.13060158478822026,
          0.22798833417562603,
          0.2447200668299837,
          0.19773583190707644,
          0.2673253617485081,
          0.2536299694216919,
          0.13752754334777673,
          0.15244343834795204,
          0.16138591281406034,
          0.33034811090554206,
          0.15213717679755456,
          0.15922989155807576,
          0.19670474961432105,
          0.1013319275571094,
          0.21810731762934843,
          0.1917559871631851,
          0.18744720091102907,
          0.25685201069064856,
          0.1805763884275715,
          0.2654183922468212,
          0.22547996051019692,
          0.15263476564557354,
          0.27827006302830604,
          0.1850585164778754,
          0.283819239015776,
          0.18707008077761345,
          0.27368626393049433,
          0.2589550969743548,
          0.22696009842826081,
          0.14093145549821828,
          0.2076279120058292,
          0.17647168814257885,
          0.15368856219507832,
          0.1984560661946062
         ],
         "xaxis": "x",
         "y": [
          0.1948197076339919,
          0.2883478435076266,
          0.14902036672601215,
          0.14307942781461164,
          0.17128114440359982,
          0.340183674099743,
          0.14055728978296694,
          0.2913106904229936,
          0.15071971059355105,
          0.13993854773144473,
          0.17981783523676212,
          0.10617286974105979,
          0.18273674831304793,
          0.16777865085331525,
          0.16202566977690658,
          0.12450789434483343,
          0.15075088470100162,
          0.21254639647753645,
          0.1459781637552776,
          0.21156860407399866,
          0.15570405101089,
          0.09151046146169464,
          0.29576284846282225,
          0.16979887013345624,
          0.2546563607405623,
          0.14235874061806125,
          0.17867403947628693,
          0.289085702365421,
          0.19628123415138615,
          0.20038624028026,
          0.1674521438003234,
          0.19050656937144053,
          0.28674745327647594,
          0.12158084790798176,
          0.15569845177543737,
          0.16952471330561295,
          0.2631872069074553,
          0.19225302864741842,
          0.32167588945876563,
          0.1616687510875496,
          0.18332481175245635,
          0.11891402573509363,
          0.19651974623394322,
          0.1901660869807868,
          0.19822264618370444,
          0.14292479667023117,
          0.15588284444926048,
          0.15480440831845851,
          0.16386533249787952,
          0.11226068097520854,
          0.18153808285015885,
          0.13331893164812003,
          0.12706400819819044,
          0.12140846017365449,
          0.18254047003955828,
          0.23184546045117999,
          0.13460381784706665,
          0.19053968212854389,
          0.22470209692537635,
          0.1577326423631332,
          0.14568919154868712,
          0.20304011854389617,
          0.12360405544137498,
          0.27859884563767306,
          0.13990675589999998,
          0.11251555799968249,
          0.25548816841152633,
          0.14867564858245805,
          0.1338246896809072,
          0.16506460907060239,
          0.11593590676729618,
          0.12793566224337305,
          0.23685207999416563,
          0.21218820306534386,
          0.19885526763604264,
          0.09536348289896675,
          0.2231686750492331,
          0.20611529913667484,
          0.16486321952751865,
          0.29286843046735395,
          0.15218662808080455,
          0.17352505099474122,
          0.17398616075380777,
          0.12842057445928068,
          0.1856242821515054,
          0.1256684962181535,
          0.1472713751259256,
          0.21548027653734342,
          0.11498581132243661,
          0.17743747258215758,
          0.11449679275374078,
          0.17538654146606958,
          0.10941778513246689,
          0.1267482495499478,
          0.1493407026238558,
          0.2276119962703254,
          0.16268136063116967,
          0.18935316310453698,
          0.21326138740887735,
          0.16875279206263563
         ],
         "yaxis": "y"
        },
        {
         "line": {
          "dash": "dash"
         },
         "mode": "lines",
         "name": "y=x",
         "type": "scatter",
         "x": [
          0,
          1
         ],
         "y": [
          0,
          1
         ]
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "In-Sample vs Out-of-Sample R-squared Performance"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "In Sample Performance (R-squared)"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Out of Sample Performance (R-squared)"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"a7cc2a0c-9290-4caf-99fa-159cf676c299\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a7cc2a0c-9290-4caf-99fa-159cf676c299\")) {                    Plotly.newPlot(                        \"a7cc2a0c-9290-4caf-99fa-159cf676c299\",                        [{\"hovertemplate\":\"In Sample Performance (R-squared)=%{x}\\u003cbr\\u003eOut of Sample Performance (R-squared)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.1678367784811714,0.11063387541129288,0.2180252435846015,0.2247685059961666,0.1930648587344811,0.08736776568684879,0.23288670034407188,0.11838142587409073,0.21932073090607662,0.23395968115467036,0.18372275075966804,0.31027605307729755,0.17446272568439936,0.19454495605237732,0.20091261610892486,0.26171058427318694,0.21353066204339566,0.15303386396357765,0.2252228876996677,0.15264261160233272,0.21455739745509594,0.3131236042002644,0.11163091513197654,0.1954061237698429,0.11945135996017253,0.2286780432439801,0.1732312337122771,0.1058249717287656,0.16885281804993102,0.16181430196730628,0.1953575872715918,0.17491554267906861,0.1054652563081736,0.2919347991723632,0.19898400701944596,0.19317604994505944,0.12150909379820063,0.17241361286553225,0.10024983253580655,0.2033775584879346,0.1806803714163251,0.2703559187986564,0.17050873655238252,0.17148768769584422,0.16655780525172026,0.22634742861243717,0.21136421183179366,0.20670943192350366,0.18716718321093018,0.26324712646007764,0.18320225374344212,0.23694459428466652,0.25630615717920613,0.2716887637087,0.18205482280932384,0.1417595388461531,0.24198061060384424,0.17648764597980138,0.1496684828877648,0.2006304701237559,0.22886425932250987,0.15381251264049933,0.25389134510647726,0.11792735972745061,0.22429211012039973,0.2939251438246523,0.13060158478822026,0.22798833417562603,0.2447200668299837,0.19773583190707644,0.2673253617485081,0.2536299694216919,0.13752754334777673,0.15244343834795204,0.16138591281406034,0.33034811090554206,0.15213717679755456,0.15922989155807576,0.19670474961432105,0.1013319275571094,0.21810731762934843,0.1917559871631851,0.18744720091102907,0.25685201069064856,0.1805763884275715,0.2654183922468212,0.22547996051019692,0.15263476564557354,0.27827006302830604,0.1850585164778754,0.283819239015776,0.18707008077761345,0.27368626393049433,0.2589550969743548,0.22696009842826081,0.14093145549821828,0.2076279120058292,0.17647168814257885,0.15368856219507832,0.1984560661946062],\"xaxis\":\"x\",\"y\":[0.1948197076339919,0.2883478435076266,0.14902036672601215,0.14307942781461164,0.17128114440359982,0.340183674099743,0.14055728978296694,0.2913106904229936,0.15071971059355105,0.13993854773144473,0.17981783523676212,0.10617286974105979,0.18273674831304793,0.16777865085331525,0.16202566977690658,0.12450789434483343,0.15075088470100162,0.21254639647753645,0.1459781637552776,0.21156860407399866,0.15570405101089,0.09151046146169464,0.29576284846282225,0.16979887013345624,0.2546563607405623,0.14235874061806125,0.17867403947628693,0.289085702365421,0.19628123415138615,0.20038624028026,0.1674521438003234,0.19050656937144053,0.28674745327647594,0.12158084790798176,0.15569845177543737,0.16952471330561295,0.2631872069074553,0.19225302864741842,0.32167588945876563,0.1616687510875496,0.18332481175245635,0.11891402573509363,0.19651974623394322,0.1901660869807868,0.19822264618370444,0.14292479667023117,0.15588284444926048,0.15480440831845851,0.16386533249787952,0.11226068097520854,0.18153808285015885,0.13331893164812003,0.12706400819819044,0.12140846017365449,0.18254047003955828,0.23184546045117999,0.13460381784706665,0.19053968212854389,0.22470209692537635,0.1577326423631332,0.14568919154868712,0.20304011854389617,0.12360405544137498,0.27859884563767306,0.13990675589999998,0.11251555799968249,0.25548816841152633,0.14867564858245805,0.1338246896809072,0.16506460907060239,0.11593590676729618,0.12793566224337305,0.23685207999416563,0.21218820306534386,0.19885526763604264,0.09536348289896675,0.2231686750492331,0.20611529913667484,0.16486321952751865,0.29286843046735395,0.15218662808080455,0.17352505099474122,0.17398616075380777,0.12842057445928068,0.1856242821515054,0.1256684962181535,0.1472713751259256,0.21548027653734342,0.11498581132243661,0.17743747258215758,0.11449679275374078,0.17538654146606958,0.10941778513246689,0.1267482495499478,0.1493407026238558,0.2276119962703254,0.16268136063116967,0.18935316310453698,0.21326138740887735,0.16875279206263563],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"y=x\",\"x\":[0,1],\"y\":[0,1],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"In Sample Performance (R-squared)\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Out of Sample Performance (R-squared)\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"In-Sample vs Out-of-Sample R-squared Performance\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a7cc2a0c-9290-4caf-99fa-159cf676c299');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load the Pokémon dataset\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokemon_data = pd.read_csv(url)\n",
    "\n",
    "# Set model formula (use model3 for simpler specifications initially)\n",
    "model_formula = 'HP ~ Attack + Defense'\n",
    "\n",
    "# Number of repetitions\n",
    "reps = 100\n",
    "in_sample_Rsquared = np.zeros(reps)\n",
    "out_of_sample_Rsquared = np.zeros(reps)\n",
    "\n",
    "# Run loop to repeatedly split data, fit model, and calculate R-squared values\n",
    "for i in range(reps):\n",
    "    # Split the data into 50-50 train-test split without random seed\n",
    "    train_data, test_data = train_test_split(pokemon_data, test_size=0.5)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model_fit = smf.ols(formula=model_formula, data=train_data).fit()\n",
    "    \n",
    "    # \"In-sample\" R-squared\n",
    "    in_sample_Rsquared[i] = model_fit.rsquared\n",
    "    \n",
    "    # \"Out-of-sample\" R-squared, based on correlation between actual and predicted HP in test set\n",
    "    out_of_sample_Rsquared[i] = np.corrcoef(test_data['HP'], model_fit.predict(test_data))[0, 1] ** 2\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "df = pd.DataFrame({\n",
    "    \"In Sample Performance (R-squared)\": in_sample_Rsquared,\n",
    "    \"Out of Sample Performance (R-squared)\": out_of_sample_Rsquared\n",
    "})\n",
    "\n",
    "# Plot the results\n",
    "fig = px.scatter(df, x=\"In Sample Performance (R-squared)\", y=\"Out of Sample Performance (R-squared)\",\n",
    "                 title=\"In-Sample vs Out-of-Sample R-squared Performance\")\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", name=\"y=x\", line=dict(dash=\"dash\")))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48308106",
   "metadata": {},
   "source": [
    "## Interpretation of Results:\n",
    "\n",
    "#### 1. Fluctuations in R-squared Values:\n",
    "Each iteration produces different \"in-sample\" and \"out-of-sample\" R-squared values due to the random split of training and testing data. These fluctuations reflect the model’s sensitivity to sample variance—especially if there are significant shifts between \"in-sample\" and \"out-of-sample\" performance.\n",
    "\n",
    "#### 2. Identifying Overfitting Patterns:\n",
    "If \"in-sample\" R-squared values are consistently higher than \"out-of-sample\" R-squared, this suggests that the model may be overfitting, learning specific patterns from the training data that do not generalize well.\n",
    "\n",
    "#### 3. Visual Analysis for Stability:\n",
    "By plotting the R-squared values, we can observe how closely they follow the y=x line. A model that generalizes well will have points closer to this line, indicating that both \"in-sample\" and \"out-of-sample\" performances are similar.\n",
    "\n",
    "#### 4. Purpose of the Demonstration:\n",
    "This exercise highlights the importance of assessing a model’s stability across different data samples. Observing consistent \"out-of-sample\" performance closer to \"in-sample\" performance suggests better generalizability, while significant drops reveal areas where the model may rely too heavily on specific data patterns, potentially due to multicollinearity or model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dcb44f",
   "metadata": {},
   "source": [
    "# ChatGPT Summary:\n",
    "\n",
    "1. **Understanding Model Complexity and Overfitting**: We discussed how a complex model (like `model4_fit`) with high multicollinearity may overfit the training data due to capturing spurious patterns. We looked at the importance of a simpler model (like `model3_fit`) for better generalization, as it is less likely to model noise specific to the training dataset.\n",
    "\n",
    "2. **Implementing Multiple Model Performance Metrics**: Following instructions to assess model generalizability, we explored a for-loop approach. This loop performs repeated 50-50 train-test splits of the Pokémon dataset to generate and collect \"in-sample\" and \"out-of-sample\" R-squared values, allowing observation of how well a simpler model specification generalizes to new data. We used Plotly to visualize the relationship between these metrics.\n",
    "\n",
    "3. **Dataset Loading Issue**: You encountered an error (`NameError: name 'pokemon_data' is not defined`) when running the loop. To resolve this, we reloaded the Pokémon dataset from the specified URL into the variable `pokemon_data` and successfully ran the loop with the updated code.\n",
    "\n",
    "4. **Final Visualization**: We created a scatter plot of in-sample versus out-of-sample R-squared values with a reference line (y=x) to visualize and interpret model generalizability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c05c7",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2d8a8b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.26967828636457025 (original)\n",
      "'Out of sample' R-squared: 0.29225032475756824 (original)\n",
      "'In sample' R-squared:     0.3482687905833055 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.14658589956997625 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "# Ensure model7_linear_form is defined first\n",
    "model7_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'  # Adjust this according to your dataset\n",
    "\n",
    "# Fit the original model (model7_fit) with the full dataset\n",
    "model7_fit = smf.ols(formula=model7_linear_form, data=pokeaman).fit()\n",
    "\n",
    "# Generate predictions (yhat_model7) for pokeaman_test data\n",
    "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
    "\n",
    "# Fit the model for Generation 1 data\n",
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form, data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "\n",
    "# Print In-sample R-squared for the original model\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "\n",
    "# Calculate and print Out-of-sample R-squared for the original model\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model7)[0, 1]**2, \"(original)\")\n",
    "\n",
    "# Print In-sample R-squared for the Generation 1 prediction model\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "\n",
    "# Calculate and print Out-of-sample R-squared for the Generation 1 prediction model\n",
    "y = pokeaman[pokeaman.Generation != 1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation != 1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat)[0, 1]**2, \"(gen1_predict_future)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1b14b",
   "metadata": {},
   "source": [
    "## First Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e6441e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.26967828636457025 (original)\n",
      "'Out of sample' R-squared: 0.29225032475756824 (original)\n",
      "'In sample' R-squared:     0.3482687905833055 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.14658589956997625 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8359eba",
   "metadata": {},
   "source": [
    "## Code Interpretation:\n",
    "### **1. In-Sample and Out-of-Sample R-squared Analysis for Model7**:\n",
    "\n",
    "- The in-sample R-squared for the original model (`model7_fit`) is relatively high, indicating that the model fits the training data well.\n",
    "\n",
    "- However, the out-of-sample R-squared for the original model, when applied to the test dataset (`pokeaman_test.HP`), is lower. This suggests that the model doesn't generalize as effectively to unseen data, which could be a sign of overfitting.\n",
    "\n",
    "- When evaluating the model trained only on **Generation 1** data (`model7_gen1_predict_future`), the in-sample R-squared is again high, which is expected since the model was trained on this data. However, when we test this model on data from `other generations`, the out-of-sample R-squared is computed. If this value is lower, it indicates that the model trained on Generation 1 data may not perform well when applied to other generations, demonstrating potential overfitting.\n",
    "\n",
    "### **2. Model Complexity vs. Generalizability**:\n",
    "\n",
    "- The high in-sample R-squared and lower out-of-sample R-squared suggest that while `model7` fits well to the training data, its generalizability may be limited, especially when tested on new data (i.e., Pokémon from later generations). This highlights the issue of overfitting that can arise with more complex models.\n",
    "\n",
    "- Overall, this emphasizes the importance of balancing **model complexity** with **generalizability.** While a more complex model may provide a better fit to the training data, it can suffer from poor performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68be68",
   "metadata": {},
   "source": [
    "## Second Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "78b369b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.26967828636457025 (original)\n",
      "'Out of sample' R-squared: 0.29225032475756824 (original)\n",
      "'In sample' R-squared:     0.2690171108300998 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.29130914074696457 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7fddc4",
   "metadata": {},
   "source": [
    "### **1. In-Sample and Out-of-Sample R-squared Analysis for Model7 (Trained on Generations 1-5):**\n",
    "\n",
    "- **In-Sample R-squared for the Original Model (`model7_fit`):**\n",
    "  The in-sample R-squared value for the original model (`model7_fit`) is reported first. This value indicates how well the original model fits the training data. A higher in-sample R-squared suggests a good fit to the training dataset, as the model explains a significant portion of the variance in the data.\n",
    "\n",
    "- **Out-of-Sample R-squared for the Original Model (`yhat_model7`):**\n",
    "  Next, we calculate the out-of-sample R-squared by comparing the predicted values (`yhat_model7`) from the original model with the true values from the test data (`pokeaman_test.HP`). A decrease in R-squared when moving from in-sample to out-of-sample implies that the model does not generalize well to unseen data, which is a classic sign of **overfitting**.\n",
    "\n",
    "- **In-Sample R-squared for Model7 Trained on Generations 1-5 (`model7_gen1to5_predict_future`):**\n",
    "  The in-sample R-squared for the model trained on Generations 1-5 (`model7_gen1to5_predict_future`) is then computed. This value indicates how well the model trained on data from Generations 1 to 5 fits those generations. A higher in-sample R-squared suggests that the model explains the variance in the data from these generations well.\n",
    "\n",
    "- **Out-of-Sample R-squared for Model7 Trained on Generations 1-5 (Tested on Generation 6 Data):**\n",
    "  We then test this model on data from Generation 6 (`pokeaman[pokeaman.Generation==6]`) and compute the out-of-sample R-squared. If this value is lower than the in-sample R-squared, it indicates that the model trained on data from Generations 1-5 does not generalize well to Generation 6. This would again highlight potential **overfitting** to the earlier generations and the model's inability to adapt to data from new generations.\n",
    "\n",
    "\n",
    "### **2. Model Complexity and Generalizability Analysis:**\n",
    "\n",
    "- **Original Model and Overfitting:**\n",
    "  The original model (`model7_fit`) shows high in-sample R-squared but lower out-of-sample R-squared when tested on the test data. This suggests overfitting to the training data and indicates that the model may not generalize well to unseen data, pointing to potential issues with **model complexity**.\n",
    "\n",
    "- **Model Trained on Generations 1-5:**\n",
    "  The model trained on Generations 1-5 (`model7_gen1to5_predict_future`) shows a similar pattern: high in-sample R-squared and lower out-of-sample R-squared when tested on Generation 6 data. This indicates that the model captures patterns in Generations 1-5 but does not generalize well to data from a different generation. \n",
    "\n",
    "- **Impact of Model Complexity:**\n",
    "  The complexity of the model is highlighted in both cases. While the in-sample performance is strong, the out-of-sample performance reveals that the model may be overfitting to specific features of the data from earlier generations. This shows the trade-off between model complexity (which leads to good fit but poor generalizability) and simplicity (which may have better generalizability but at the cost of a poorer fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014b0be",
   "metadata": {},
   "source": [
    "# Third Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee040b2",
   "metadata": {},
   "source": [
    "The code is keep generating an error, so I commented out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "48e5fb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#', 'Name', 'Type 1', 'Type 2', 'HP', 'Attack', 'Defense', 'Sp. Atk',\n",
       "       'Sp. Def', 'Speed', 'Generation', 'Legendary', 'str8fyre'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the column names in the dataset\n",
    "pokeaman.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3eda0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "#                                      data=pokeaman[pokeaman.Generation==1])\n",
    "# model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "\n",
    "# print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "\n",
    "# y = pokeaman_test.HP\n",
    "# print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model6)[0, 1]**2, \"(original)\")\n",
    "\n",
    "# print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "\n",
    "# y = pokeaman[pokeaman.Generation != 1].HP\n",
    "# yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation != 1])\n",
    "# print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat)[0, 1]**2, \"(gen1_predict_future)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8da47d",
   "metadata": {},
   "source": [
    "### **1. In-Sample and Out-of-Sample R-squared Analysis for Model6 (Trained on Generation 1 Data):**\n",
    "\n",
    "#### **In-Sample R-squared for the Original Model (`model6_fit`):**\n",
    "- The in-sample R-squared for the original model (`model6_fit`) represents how well the model fits the training data. A higher R-squared value indicates that the model explains a large proportion of the variance in the dependent variable (HP in this case) for the data it was trained on.\n",
    "- The in-sample R-squared for the original model provides an indication of how well the model captures patterns within the data it has seen. It tells us about the model's performance on the training set.\n",
    "\n",
    "#### **Out-of-Sample R-squared for the Original Model (`yhat_model6`):**\n",
    "- The out-of-sample R-squared is calculated by comparing the predicted values (`yhat_model6`) from the original model with the actual values (`pokeaman_test.HP`) from the test set.\n",
    "- A lower out-of-sample R-squared compared to the in-sample R-squared indicates that the model struggles to generalize to unseen data. This would point to **overfitting**, meaning the model fits the training data well but does not perform as well on new, unseen data. It is important to assess whether the model's good performance is just due to memorization of the training set (overfitting) or whether it reflects generalizable patterns.\n",
    "\n",
    "#### **In-Sample R-squared for Model6 Trained on Generation 1 Data (`model6_gen1_predict_future`):**\n",
    "- This step fits a model on just Generation 1 data (`pokeaman[Generation==1]`) and calculates the in-sample R-squared for this specific model. This R-squared value reflects how well the model explains the variance of HP within Generation 1 alone.\n",
    "- A high in-sample R-squared would suggest that the model explains the variability within Generation 1 well. However, it's crucial to note that this performance may not extend to other generations of Pokémon.\n",
    "\n",
    "#### **Out-of-Sample R-squared for Model6 Trained on Generation 1 Data (Tested on Other Generations):**\n",
    "- In this step, the model trained on Generation 1 data is tested on the data from other generations (`pokeaman[Generation!=1]`).\n",
    "- The out-of-sample R-squared is calculated by comparing the predicted HP values for other generations (`yhat`) with the actual HP values from those generations (`y`).\n",
    "- A lower out-of-sample R-squared for this test suggests that the model trained solely on Generation 1 data does not generalize well to data from other generations. This could indicate **overfitting** to Generation 1 and a lack of adaptability to the patterns of Pokémon from other generations.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Model Complexity vs. Generalizability:**\n",
    "\n",
    "#### **Overfitting and Limited Generalizability:**\n",
    "- Just like in the previous examples, a high in-sample R-squared paired with a low out-of-sample R-squared suggests that the model is **overfitting** to the training data and may not generalize well to new data.\n",
    "- In this case, the model trained on Generation 1 data has a good fit for that specific subset of the data (high in-sample R-squared), but its performance significantly drops when tested on data from other generations (lower out-of-sample R-squared). This shows that the model may have captured the specific patterns of Generation 1 but is unable to adapt to the diversity in the data from other generations.\n",
    "\n",
    "#### **Generalizability Across Generations:**\n",
    "- When the model trained on Generation 1 data is applied to data from other generations, we see the limitations in terms of generalizability. This suggests that the patterns learned from Generation 1 data are not necessarily applicable or robust enough to handle data from different generations.\n",
    "- If the out-of-sample R-squared is significantly lower for this test, it supports the idea that the model's ability to generalize to new data (Pokémon from other generations) is limited. This indicates that the model's complexity might have made it too tailored to the characteristics of Generation 1, leading to poor performance when applied to data outside that generation.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Conclusion:**\n",
    "- **Overfitting**: As observed, both the original model and the model trained on Generation 1 data perform well on the training data but fail to generalize to new data (test or other generations). This highlights a classic case of **overfitting**, where the model is too complex for the data and learns idiosyncratic patterns that don't generalize.\n",
    "- **Model Complexity and Generalizability**: This analysis underscores the trade-off between model complexity (which leads to high in-sample performance) and generalizability (which can suffer when the model becomes too specific to the training set). Overfitting can result in poor performance on new, unseen data, which is crucial to consider when evaluating the effectiveness of any model.\n",
    "\n",
    "---\n",
    "\n",
    "This pattern of high in-sample R-squared and low out-of-sample R-squared shows the importance of balancing the fit of the model on the training data with its ability to perform well on new, unseen data. **Cross-validation** or training models on a wider variety of data, such as across multiple generations, could help improve generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f877de67",
   "metadata": {},
   "source": [
    "# Fourth Code:\n",
    "Commented out since the code keep generated an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0aa1d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "#                                        data=pokeaman[pokeaman.Generation!=6])\n",
    "# model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "# print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "# y = pokeaman_test.HP\n",
    "# print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "# print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "# y = pokeaman[pokeaman.Generation==6].HP\n",
    "# yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "# print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f13fe",
   "metadata": {},
   "source": [
    "### **1. In-Sample and Out-of-Sample R-squared Analysis for Model7 (Trained on Generations 1-5):**\n",
    "\n",
    "- **In-Sample R-squared for the Original Model (`model7_fit`):**\n",
    "  The in-sample R-squared value for the original model (`model7_fit`) indicates how well the model fits the training data. A higher in-sample R-squared means that the model is able to explain a large proportion of the variance in the target variable (`HP`) within the training data. If this value is high, it suggests that the model has learned the relationships between the predictors and the target effectively for the data it was trained on.\n",
    "\n",
    "- **Out-of-Sample R-squared for the Original Model (`yhat_model7`):**\n",
    "  The out-of-sample R-squared is calculated by comparing the predicted values (`yhat_model7`) from the original model with the actual values from the test dataset (`pokeaman_test.HP`). A decrease in R-squared when moving from in-sample to out-of-sample indicates that the model does not generalize well to unseen data. This is a potential sign of **overfitting**, where the model has learned the specifics of the training data too well, including noise or irrelevant patterns, which hampers its ability to predict unseen data accurately.\n",
    "\n",
    "- **In-Sample R-squared for Model7 Trained on Generations 1-5 (`model7_gen1to5_predict_future`):**\n",
    "  The in-sample R-squared for the model trained on Generations 1-5 (`model7_gen1to5_predict_future`) measures how well the model explains the variance in the data from these specific generations. A high in-sample R-squared indicates that the model fits the data from Generations 1-5 well, capturing the underlying patterns in the training set. However, it is important to note that a good fit on training data does not necessarily mean that the model will generalize well to new, unseen data.\n",
    "\n",
    "- **Out-of-Sample R-squared for Model7 Trained on Generations 1-5 (Tested on Generation 6 Data):**\n",
    "  The out-of-sample R-squared for the model trained on Generations 1-5 is calculated by testing the model on Generation 6 data. If this R-squared value is lower than the in-sample R-squared, it suggests that the model trained on earlier generations (1-5) does not generalize well to the data from Generation 6. This again points to potential **overfitting** to the earlier generations and a lack of flexibility or adaptability in the model when applied to data from a new generation.\n",
    "\n",
    "\n",
    "### **2. Model Complexity and Generalizability Analysis:**\n",
    "\n",
    "- **Original Model and Overfitting:**\n",
    "  The original model (`model7_fit`) shows a strong in-sample R-squared but a significant drop in out-of-sample R-squared when tested on the test data. This suggests that the model has overfitted to the training data. Overfitting occurs when a model learns too many details and noise in the training data, which leads to poor performance on new, unseen data. The decrease in R-squared between in-sample and out-of-sample predictions highlights that the model is too complex and fails to generalize well to other data.\n",
    "\n",
    "- **Model Trained on Generations 1-5:**\n",
    "  The model trained on Generations 1-5 (`model7_gen1to5_predict_future`) exhibits a similar pattern, with high in-sample R-squared but a lower out-of-sample R-squared when applied to Generation 6 data. This further reinforces the issue of overfitting, as the model captures the patterns in Generations 1-5 well but struggles to apply that knowledge to Generation 6. The drop in R-squared indicates that the model’s predictive ability is limited when faced with new data that differs from the training set.\n",
    "\n",
    "- **Impact of Model Complexity:**\n",
    "  In both cases, the complexity of the model is evident. While the in-sample performance is strong, the out-of-sample performance reveals that the model is not generalizing well to new data. This illustrates the trade-off between model complexity and generalizability:\n",
    "  \n",
    "  - A **complex model** may have excellent in-sample performance by fitting the training data very well, but it may overfit and struggle with new data, leading to poor generalization.\n",
    "  - A **simpler model**, which might not fit the training data as perfectly, may generalize better and perform more consistently across different datasets.\n",
    "\n",
    "  The analysis suggests that while a complex model may seem attractive because of its high in-sample fit, the lack of generalizability signals that model complexity needs to be carefully managed to avoid overfitting and ensure better predictive performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff218ba4",
   "metadata": {},
   "source": [
    "# ChatGPT Summary\n",
    "\n",
    "#### Key Points Discussed:\n",
    "\n",
    "1. **In-Sample vs. Out-of-Sample R-squared Values:**\n",
    "   - I learned how to calculate and interpret the in-sample R-squared, which indicates how well a model fits the training data. A higher in-sample R-squared suggests a better fit to the training data.\n",
    "   - I also explored how to calculate the out-of-sample R-squared, which compares the predicted values from the model to the actual values from a test dataset. A significant drop in out-of-sample R-squared indicates that the model may be overfitting to the training data, as it struggles to generalize to unseen data.\n",
    "\n",
    "2. **Overfitting and Model Complexity:**\n",
    "   - Through the discussion, I understood that models with high in-sample R-squared but lower out-of-sample R-squared values are likely overfitting the data. This means the model has captured noise or irrelevant patterns in the training data, which reduces its ability to predict accurately on new data.\n",
    "   - I explored how overfitting can result from complex models that fit the training data too closely, highlighting the importance of balancing model complexity with generalizability.\n",
    "\n",
    "3. **Impact of Model Training on Different Generations:**\n",
    "   - We also discussed training models on different generations of Pokémon data (Generations 1-5) and testing their ability to generalize to data from Generation 6. This analysis revealed the challenge of creating models that can perform well across different datasets, suggesting the need for caution in using models trained on specific subsets of data.\n",
    "\n",
    "4. **Model Generalization:**\n",
    "   - The analysis underscored the trade-off between model complexity and generalization. A simpler model, though it may not fit the training data as perfectly, could generalize better across different datasets, which is critical in predictive modeling tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
